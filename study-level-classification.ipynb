{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:37:24.156061Z",
     "iopub.status.busy": "2021-07-09T16:37:24.155702Z",
     "iopub.status.idle": "2021-07-09T16:37:24.160246Z",
     "shell.execute_reply": "2021-07-09T16:37:24.159057Z",
     "shell.execute_reply.started": "2021-07-09T16:37:24.156028Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pylibjpeg gdcm\n",
    "#!pip install tensorflow==2.5\n",
    "# !pip  install pylibjpeg-libjpeg\n",
    "# !pip install numpy==1.21\n",
    "#!pip install tensorflow-gpu==2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:04.646343Z",
     "iopub.status.busy": "2021-07-09T16:38:04.645873Z",
     "iopub.status.idle": "2021-07-09T16:38:04.654473Z",
     "shell.execute_reply": "2021-07-09T16:38:04.652978Z",
     "shell.execute_reply.started": "2021-07-09T16:38:04.646297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pathlib\n",
    "import warnings\n",
    "import logging\n",
    "import PIL\n",
    "from numba import cuda\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "effnet_path = os.path.join(os.getcwd(), 'automl', 'efficientnetv2')\n",
    "sys.path.append(effnet_path)\n",
    "import effnetv2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:07.02978Z",
     "iopub.status.busy": "2021-07-09T16:38:07.029378Z",
     "iopub.status.idle": "2021-07-09T16:38:07.03634Z",
     "shell.execute_reply": "2021-07-09T16:38:07.034959Z",
     "shell.execute_reply.started": "2021-07-09T16:38:07.029734Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('D:\\Datasets\\siim_covid19_detection')\n",
    "df = pd.read_csv(os.path.join(data_dir,'study.csv'))\n",
    "label_columns = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:07.362027Z",
     "iopub.status.busy": "2021-07-09T16:38:07.361575Z",
     "iopub.status.idle": "2021-07-09T16:38:07.425105Z",
     "shell.execute_reply": "2021-07-09T16:38:07.423655Z",
     "shell.execute_reply.started": "2021-07-09T16:38:07.361994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4799 1200\n",
      "4799 1200\n",
      "4799 1200\n",
      "4799 1200\n",
      "4800 1199\n"
     ]
    }
   ],
   "source": [
    "#create folds\n",
    "gkf  = GroupKFold(n_splits = 5)\n",
    "df['fold'] = -1\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups = df.index.tolist())):\n",
    "    df.loc[val_idx, 'fold'] = fold\n",
    "    print(len(train_idx), len(val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:07.5249Z",
     "iopub.status.busy": "2021-07-09T16:38:07.524486Z",
     "iopub.status.idle": "2021-07-09T16:38:07.533771Z",
     "shell.execute_reply": "2021-07-09T16:38:07.532402Z",
     "shell.execute_reply.started": "2021-07-09T16:38:07.524868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process an image path\n",
    "def process_path(file_path):\n",
    "    image_name = tf.strings.split(str(file_path), os.sep).numpy()[-1].decode()\n",
    "    image_id = image_name.split('_')[0]\n",
    "\n",
    "    label = tf.cast((df.loc[df['StudyInstanceUID'] == image_id+'_study'][label_columns]).values[0], dtype = tf.int16)\n",
    "    img = tf.io.read_file(file_path.numpy().decode())\n",
    "    img = tf.image.decode_png(img, channels = 1)\n",
    "    img =  tf.image.grayscale_to_rgb(img)\n",
    "    if img.numpy().max() > 255:\n",
    "        img = img/65535\n",
    "    else:\n",
    "        img = img/255\n",
    "    # Gamma Correction\n",
    "#     if img.numpy().max() < 0.49:\n",
    "#         tf.image.adjust_gamma(img, 0.7)\n",
    "#     elif img.numpy().min() > 0.5:\n",
    "#         tf.image.adjust_gamma(img, 3)\n",
    "    # img = tf.cast(img, dtype = tf.int16)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:07.684631Z",
     "iopub.status.busy": "2021-07-09T16:38:07.684244Z",
     "iopub.status.idle": "2021-07-09T16:38:08.035364Z",
     "shell.execute_reply": "2021-07-09T16:38:08.034205Z",
     "shell.execute_reply.started": "2021-07-09T16:38:07.684599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6054"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing all the file names\n",
    "train_dir = pathlib.Path(os.path.join(data_dir, '320px','train','study'))\n",
    "train_image_paths = list(train_dir.glob('*.png'))\n",
    "\n",
    "len(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:08.037735Z",
     "iopub.status.busy": "2021-07-09T16:38:08.037281Z",
     "iopub.status.idle": "2021-07-09T16:38:12.622283Z",
     "shell.execute_reply": "2021-07-09T16:38:12.621133Z",
     "shell.execute_reply.started": "2021-07-09T16:38:08.03767Z"
    }
   },
   "outputs": [],
   "source": [
    "train_image_paths = [path for path in train_image_paths if str(path).split(os.sep)[-1].split('.')[0] in list(df.StudyInstanceUID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:12.625719Z",
     "iopub.status.busy": "2021-07-09T16:38:12.625249Z",
     "iopub.status.idle": "2021-07-09T16:38:12.632077Z",
     "shell.execute_reply": "2021-07-09T16:38:12.63064Z",
     "shell.execute_reply.started": "2021-07-09T16:38:12.625656Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paths(ids, all_paths):\n",
    "    paths = [path for path in all_paths if str(path).split(os.sep)[-1].split('.')[0] in ids]\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:12.634733Z",
     "iopub.status.busy": "2021-07-09T16:38:12.634184Z",
     "iopub.status.idle": "2021-07-09T16:38:12.647035Z",
     "shell.execute_reply": "2021-07-09T16:38:12.645895Z",
     "shell.execute_reply.started": "2021-07-09T16:38:12.634642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create data pipeline\n",
    "def input_pipeline(train_paths, val_paths):\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices([str(path) for path in train_paths])\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices([str(path) for path in val_paths])\n",
    "    # test_ds = tf.data.Dataset.list_files(str(dataDir/'test/*/*/*.dcm'), shuffle=True)\n",
    "    train_final_ds = train_ds.map(lambda x: tf.py_function(func = process_path,inp = [x], Tout = (tf.float32, tf.int16)), num_parallel_calls = tf.data.AUTOTUNE)\n",
    "    valid_ds = val_ds.map(lambda x: tf.py_function(func = process_path,inp = [x], Tout = (tf.float32, tf.int16)), num_parallel_calls = tf.data.AUTOTUNE)\n",
    "    # test_ds = test_ds.map(lambda x: tf.py_function(func = process_path,inp = [x], Tout = (tf.float32, tf.int16)), num_parallel_calls = tf.data.AUTOTUNE)\n",
    "    train = train_final_ds.batch(batchSize).prefetch(tf.data.AUTOTUNE)\n",
    "    valid = valid_ds.batch(batchSize).prefetch(tf.data.AUTOTUNE)\n",
    "    # test = test_ds.batch(batchSize).prefetch(tf.data.AUTOTUNE)\n",
    "    return train, valid#, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(history):\n",
    "    best_stats ={}\n",
    "    best_val_loss_index = np.argmin(history.history['val_loss'])\n",
    "    for key, value in history.history.items():\n",
    "        if 'val_auc' in key:\n",
    "            best_stats['val_auc'] = round(value[best_val_loss_index], 4)\n",
    "        elif 'auc' in key:\n",
    "            best_stats['auc'] = round(value[best_val_loss_index], 4)\n",
    "        else:\n",
    "            best_stats[key] = round(value[best_val_loss_index], 4)\n",
    "    return best_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain best stats\n",
    "def get_best_stats(all_folds_best_stats, folds):\n",
    "    best_stats = {}\n",
    "    for fold_key, fold_value in all_folds_best_stats.items():\n",
    "        for key, value in fold_value.items():\n",
    "            if key not in best_stats.keys():\n",
    "                best_stats[key] = value\n",
    "            else:\n",
    "                best_stats[key] += value\n",
    "    best_stats = {key: value/folds for key, value in best_stats.items()}\n",
    "    return best_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T17:01:23.890761Z",
     "iopub.status.busy": "2021-07-09T17:01:23.890348Z",
     "iopub.status.idle": "2021-07-09T17:01:23.907073Z",
     "shell.execute_reply": "2021-07-09T17:01:23.905759Z",
     "shell.execute_reply.started": "2021-07-09T17:01:23.89072Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(baseline_model, max_lr, epochs, folds, chkp_path, dropout_rate = 0.5, label_smoothing = 0, data_aug = False, img_size= 320, channels = 1):\n",
    "    all_folds_best_stats = {}\n",
    "    for fold in range(folds):\n",
    "        print(f'\\n********Training the model with validation fold {fold}********\\n')\n",
    "        # Train and validation file names for each fold\n",
    "        train_image_ids = list(df[df['fold'] != fold]['StudyInstanceUID'])\n",
    "        val_image_ids = list(df[df['fold'] == fold]['StudyInstanceUID'])\n",
    "        train_paths = get_paths(train_image_ids, train_image_paths)\n",
    "        val_paths = get_paths(val_image_ids, train_image_paths)\n",
    "        train, valid = input_pipeline(train_paths, val_paths)\n",
    "        # Define Model\n",
    "        model = tf.keras.models.Sequential([])\n",
    "        model.add(tf.keras.layers.InputLayer((img_size, img_size, channels)))\n",
    "        if data_aug:\n",
    "            model.add(tf.keras.layers.experimental.preprocessing.RandomFlip(mode = 'horizontal'))\n",
    "        model.add(baseline_model)\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "        model.add(tf.keras.layers.Dense(4, activation = 'sigmoid', name = 'sigmoidLayer'))\n",
    "\n",
    "        #Learning Rate scheduler\n",
    "#         learning_rate_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=max_lr, \n",
    "#                                                       decay_steps=150,\n",
    "#                                                       decay_rate = 0.95,\n",
    "#                                                      )\n",
    "\n",
    "        # CosineDecayRestarts\n",
    "        learning_rate_scheduler = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=max_lr, \n",
    "                                                      first_decay_steps=100,\n",
    "                                                      alpha = 0.05,\n",
    "                                                      m_mul=0.9, \n",
    "                                                      t_mul = 2  \n",
    "                                                     )\n",
    "        # Optimizer\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate_scheduler)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.0), metrics = [tf.keras.metrics.AUC()])\n",
    "        \n",
    "        if fold == 0:\n",
    "            print(model.summary())\n",
    "        \n",
    "        # Callbacks\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'checkpoints/siim/train/{chkp_path}/model_{fold}', save_best_only=True, monitor = \"val_loss\")\n",
    "        checkpoint._supports_tf_logs = False\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(min_delta = 0.0001, patience = 2)\n",
    "        \n",
    "        # Fitting the model\n",
    "        history = model.fit(train,validation_data=valid, epochs = epochs,  workers = -1,callbacks= [ early_stopping, checkpoint],verbose = 1)\n",
    "        \n",
    "        # Getting the best models results\n",
    "        fold_best_stats = get_stats(history)\n",
    "        \n",
    "        # Storing in a global dictionary to get aggregate results\n",
    "        all_folds_best_stats[f'Fold_{fold}'] = fold_best_stats\n",
    "    \n",
    "    # Get the aggregate auc and loss\n",
    "    best_stats = get_best_stats(all_folds_best_stats, folds)\n",
    "    \n",
    "    return best_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_model_input_size(pretrained_model, input_height, input_width):\n",
    "    config = pretrained_model.get_config()\n",
    "    new_shape = (None, input_height, input_width, 1)\n",
    "    \n",
    "    config['layers'][0]['config']['batch_input_shape'] = new_shape\n",
    "    config['layers'][1]['config']['layers'][0]['config']['batch_input_shape'] = new_shape\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model = model.from_config(config)\n",
    "\n",
    "    for layer in model.layers[0].layers:\n",
    "            try:\n",
    "                layer.set_weights(pretrained_model.layers[0].get_layer(name=layer.name).get_weights())\n",
    "                print(\"Loaded layer {}\".format(layer.name))\n",
    "            except:\n",
    "                print(\"Could not transfer weights for layer {}\".format(layer.name))\n",
    "    return model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet-B5 320px pretrained on 320px chexpert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the file names\n",
    "    \n",
    "batchSize = 4\n",
    "input_height, input_width = (320,320)\n",
    "train_dir = pathlib.Path(os.path.join(data_dir, f'{input_height}px','train','study'))\n",
    "train_image_paths = list(train_dir.glob('*.png'))\n",
    "train_image_paths = [path for path in train_image_paths if str(path).split(os.sep)[-1].split('.')[0] in list(df.StudyInstanceUID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block1c_activation_layer_call_and_return_conditional_losses_67372447) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7b_activation_layer_call_and_return_conditional_losses_67456842) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_67444264) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_67438322) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6g_expand_activation_layer_call_and_return_conditional_losses_67378813) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5g_activation_layer_call_and_return_conditional_losses_67450853) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3c_se_reduce_layer_call_and_return_conditional_losses_67374126) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_67437060) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5f_activation_layer_call_and_return_conditional_losses_67377197) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_67377651) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1c_se_reduce_layer_call_and_return_conditional_losses_67372482) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2e_expand_activation_layer_call_and_return_conditional_losses_67373436) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1b_se_reduce_layer_call_and_return_conditional_losses_67437413) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2d_activation_layer_call_and_return_conditional_losses_67439934) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6f_expand_activation_layer_call_and_return_conditional_losses_67378604) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_67441590) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4e_expand_activation_layer_call_and_return_conditional_losses_67375498) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2c_se_reduce_layer_call_and_return_conditional_losses_67373095) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_67375080) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_67438872) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6h_expand_activation_layer_call_and_return_conditional_losses_67379022) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4g_activation_layer_call_and_return_conditional_losses_67375958) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2c_expand_activation_layer_call_and_return_conditional_losses_67439242) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_67373723) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5g_se_reduce_layer_call_and_return_conditional_losses_67377441) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_67440901) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2d_expand_activation_layer_call_and_return_conditional_losses_67439795) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2e_se_reduce_layer_call_and_return_conditional_losses_67373513) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4g_se_reduce_layer_call_and_return_conditional_losses_67447073) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3d_expand_activation_layer_call_and_return_conditional_losses_67374258) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_67425514) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2e_activation_layer_call_and_return_conditional_losses_67373478) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3d_se_reduce_layer_call_and_return_conditional_losses_67374335) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1b_se_reduce_layer_call_and_return_conditional_losses_67372323) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_67451912) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_67374948) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1b_activation_layer_call_and_return_conditional_losses_67437369) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_67452326) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6i_expand_activation_layer_call_and_return_conditional_losses_67455644) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_67372102) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_67444861) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_67372809) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block6f_se_reduce_layer_call_and_return_conditional_losses_67378681) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6f_activation_layer_call_and_return_conditional_losses_67378646) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_67443619) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7b_expand_activation_layer_call_and_return_conditional_losses_67379634) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5d_expand_activation_layer_call_and_return_conditional_losses_67376737) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3c_expand_activation_layer_call_and_return_conditional_losses_67374049) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6f_activation_layer_call_and_return_conditional_losses_67454124) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5d_expand_activation_layer_call_and_return_conditional_losses_67449055) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4f_se_reduce_layer_call_and_return_conditional_losses_67446520) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_67443802) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_67372179) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_67451406) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_67448502) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5d_se_reduce_layer_call_and_return_conditional_losses_67376814) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3c_activation_layer_call_and_return_conditional_losses_67442099) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6g_activation_layer_call_and_return_conditional_losses_67454677) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_67444308) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3e_activation_layer_call_and_return_conditional_losses_67443205) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_67376570) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_67452465) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5d_activation_layer_call_and_return_conditional_losses_67449194) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4f_expand_activation_layer_call_and_return_conditional_losses_67446337) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6g_activation_layer_call_and_return_conditional_losses_67378855) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6i_activation_layer_call_and_return_conditional_losses_67455783) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5f_se_reduce_layer_call_and_return_conditional_losses_67450344) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_67375122) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4d_activation_layer_call_and_return_conditional_losses_67375331) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6g_se_reduce_layer_call_and_return_conditional_losses_67454721) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2d_se_reduce_layer_call_and_return_conditional_losses_67439978) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7c_se_reduce_layer_call_and_return_conditional_losses_67379920) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6f_se_reduce_layer_call_and_return_conditional_losses_67454168) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2c_se_reduce_layer_call_and_return_conditional_losses_67439425) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4d_se_reduce_layer_call_and_return_conditional_losses_67445414) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_67444678) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_67456197) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_67457809) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4e_expand_activation_layer_call_and_return_conditional_losses_67445784) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_67374754) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3e_se_reduce_layer_call_and_return_conditional_losses_67443249) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_67377616) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2c_expand_activation_layer_call_and_return_conditional_losses_67373018) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6e_expand_activation_layer_call_and_return_conditional_losses_67453432) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_67437016) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4g_se_reduce_layer_call_and_return_conditional_losses_67375993) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6e_se_reduce_layer_call_and_return_conditional_losses_67378472) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7c_se_reduce_layer_call_and_return_conditional_losses_67457439) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5d_activation_layer_call_and_return_conditional_losses_67376779) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5g_expand_activation_layer_call_and_return_conditional_losses_67450714) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4d_expand_activation_layer_call_and_return_conditional_losses_67375289) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_67377845) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3c_activation_layer_call_and_return_conditional_losses_67374091) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5g_activation_layer_call_and_return_conditional_losses_67377406) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7b_expand_activation_layer_call_and_return_conditional_losses_67456703) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_67379440) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_67448641) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_67373645) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_67377573) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4f_activation_layer_call_and_return_conditional_losses_67446476) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_67373688) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5e_se_reduce_layer_call_and_return_conditional_losses_67449791) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4d_activation_layer_call_and_return_conditional_losses_67445370) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1c_activation_layer_call_and_return_conditional_losses_67437769) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6e_expand_activation_layer_call_and_return_conditional_losses_67378395) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4f_activation_layer_call_and_return_conditional_losses_67375749) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_67452509) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5e_expand_activation_layer_call_and_return_conditional_losses_67376946) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_67378263) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7b_se_reduce_layer_call_and_return_conditional_losses_67379711) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_67438183) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_67372614) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4e_activation_layer_call_and_return_conditional_losses_67445923) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6i_expand_activation_layer_call_and_return_conditional_losses_67379231) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3c_expand_activation_layer_call_and_return_conditional_losses_67441960) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_67379482) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6g_se_reduce_layer_call_and_return_conditional_losses_67378890) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3d_activation_layer_call_and_return_conditional_losses_67374300) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_67373882) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3e_se_reduce_layer_call_and_return_conditional_losses_67374544) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_67375157) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5f_expand_activation_layer_call_and_return_conditional_losses_67450161) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_67373840) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3d_expand_activation_layer_call_and_return_conditional_losses_67442513) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4e_se_reduce_layer_call_and_return_conditional_losses_67445967) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_67443758) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_67451450) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4e_activation_layer_call_and_return_conditional_losses_67375540) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7c_expand_activation_layer_call_and_return_conditional_losses_67379843) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_67378019) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6e_activation_layer_call_and_return_conditional_losses_67453571) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1c_se_reduce_layer_call_and_return_conditional_losses_67437813) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_67456380) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_67441407) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_67379517) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6h_activation_layer_call_and_return_conditional_losses_67455230) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5f_activation_layer_call_and_return_conditional_losses_67450300) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_67441040) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2e_activation_layer_call_and_return_conditional_losses_67440487) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6h_expand_activation_layer_call_and_return_conditional_losses_67455091) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_67380052) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5g_se_reduce_layer_call_and_return_conditional_losses_67450897) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3c_se_reduce_layer_call_and_return_conditional_losses_67442143) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_67447949) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5f_se_reduce_layer_call_and_return_conditional_losses_67377232) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4d_se_reduce_layer_call_and_return_conditional_losses_67375366) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_67441084) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_67453062) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_67374871) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6h_se_reduce_layer_call_and_return_conditional_losses_67455274) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6e_se_reduce_layer_call_and_return_conditional_losses_67453615) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4g_expand_activation_layer_call_and_return_conditional_losses_67375916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_67376361) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_67447443) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_67376396) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_67378054) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2d_activation_layer_call_and_return_conditional_losses_67373269) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_67453018) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_67447626) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7c_activation_layer_call_and_return_conditional_losses_67457395) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3d_activation_layer_call_and_return_conditional_losses_67442652) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4g_expand_activation_layer_call_and_return_conditional_losses_67446890) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2e_se_reduce_layer_call_and_return_conditional_losses_67440531) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4d_expand_activation_layer_call_and_return_conditional_losses_67445231) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1b_activation_layer_call_and_return_conditional_losses_67372288) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb5_layer_call_and_return_conditional_losses_67432331) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb5_layer_call_and_return_conditional_losses_67436666) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_67376605) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_67378228) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_67438828) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_67452879) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7c_expand_activation_layer_call_and_return_conditional_losses_67457256) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_67438366) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7b_activation_layer_call_and_return_conditional_losses_67379676) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_67421161) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4f_expand_activation_layer_call_and_return_conditional_losses_67375707) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2c_activation_layer_call_and_return_conditional_losses_67373060) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5e_expand_activation_layer_call_and_return_conditional_losses_67449608) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_67377810) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_67451956) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2d_expand_activation_layer_call_and_return_conditional_losses_67373227) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7b_se_reduce_layer_call_and_return_conditional_losses_67456886) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_67447582) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_67448088) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3e_activation_layer_call_and_return_conditional_losses_67374509) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_67377977) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_67374676) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6h_activation_layer_call_and_return_conditional_losses_67379064) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_67356139) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_67377768) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_67448685) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_67372144) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_67436877) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5f_expand_activation_layer_call_and_return_conditional_losses_67377155) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_67376528) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3e_expand_activation_layer_call_and_return_conditional_losses_67374467) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_67376125) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_67376167) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6i_se_reduce_layer_call_and_return_conditional_losses_67455827) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5g_expand_activation_layer_call_and_return_conditional_losses_67377364) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3d_se_reduce_layer_call_and_return_conditional_losses_67442696) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5e_activation_layer_call_and_return_conditional_losses_67376988) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_67372657) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_67376202) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_67448132) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4e_se_reduce_layer_call_and_return_conditional_losses_67375575) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_67374719) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_67378186) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6e_activation_layer_call_and_return_conditional_losses_67378437) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7c_activation_layer_call_and_return_conditional_losses_67379885) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5e_se_reduce_layer_call_and_return_conditional_losses_67377023) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_67438689) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block2e_expand_activation_layer_call_and_return_conditional_losses_67440348) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4g_activation_layer_call_and_return_conditional_losses_67447029) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2d_se_reduce_layer_call_and_return_conditional_losses_67373304) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3e_expand_activation_layer_call_and_return_conditional_losses_67443066) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_67373917) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_67451267) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_67372692) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_67456336) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_67444125) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_67441546) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_67372851) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6h_se_reduce_layer_call_and_return_conditional_losses_67379099) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6f_expand_activation_layer_call_and_return_conditional_losses_67453985) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6g_expand_activation_layer_call_and_return_conditional_losses_67454538) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2c_activation_layer_call_and_return_conditional_losses_67439381) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5e_activation_layer_call_and_return_conditional_losses_67449747) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4f_se_reduce_layer_call_and_return_conditional_losses_67375784) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6i_se_reduce_layer_call_and_return_conditional_losses_67379308) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_67372886) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_67451773) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5d_se_reduce_layer_call_and_return_conditional_losses_67449238) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_67444817) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_67376319) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_67374913) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6i_activation_layer_call_and_return_conditional_losses_67379273) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = tf.keras.models.load_model(r'.\\checkpoints\\train\\efficientNetB5\\320px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_4 (Sequential)    (None, 51200)             28512659  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 204804    \n",
      "=================================================================\n",
      "Total params: 28,717,463\n",
      "Trainable params: 28,544,724\n",
      "Non-trainable params: 172,739\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 400s 321ms/step - loss: 1.3699 - auc_2: 0.6620 - val_loss: 1.1309 - val_auc_2: 0.7201\n",
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 382s 318ms/step - loss: 1.2033 - auc_2: 0.6502 - val_loss: 1.1160 - val_auc_2: 0.7474\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 384s 320ms/step - loss: 1.1862 - auc_2: 0.6550 - val_loss: 1.1063 - val_auc_2: 0.7454\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 379s 316ms/step - loss: 1.1734 - auc_2: 0.6557 - val_loss: 1.0996 - val_auc_2: 0.7413\n",
      "Epoch 5/15\n",
      "1200/1200 [==============================] - 379s 315ms/step - loss: 1.1308 - auc_2: 0.6701 - val_loss: 1.0897 - val_auc_2: 0.7491\n",
      "Epoch 6/15\n",
      "1200/1200 [==============================] - 377s 314ms/step - loss: 1.1673 - auc_2: 0.6688 - val_loss: 1.1227 - val_auc_2: 0.7332\n",
      "Epoch 7/15\n",
      "1200/1200 [==============================] - 378s 314ms/step - loss: 1.1328 - auc_2: 0.6683 - val_loss: 1.1253 - val_auc_2: 0.7299\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 390s 313ms/step - loss: 1.3096 - auc_3: 0.6660 - val_loss: 1.1605 - val_auc_3: 0.6617\n",
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 376s 313ms/step - loss: 1.1808 - auc_3: 0.6529 - val_loss: 1.1437 - val_auc_3: 0.6906\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 377s 314ms/step - loss: 1.1529 - auc_3: 0.6636 - val_loss: 1.1290 - val_auc_3: 0.7154\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 375s 313ms/step - loss: 1.1541 - auc_3: 0.6642 - val_loss: 1.1228 - val_auc_3: 0.7194\n",
      "Epoch 5/15\n",
      "1200/1200 [==============================] - 376s 313ms/step - loss: 1.1147 - auc_3: 0.6745 - val_loss: 1.1016 - val_auc_3: 0.7256\n",
      "Epoch 6/15\n",
      "1200/1200 [==============================] - 376s 313ms/step - loss: 1.1189 - auc_3: 0.6731 - val_loss: 1.1039 - val_auc_3: 0.7328\n",
      "Epoch 7/15\n",
      "1200/1200 [==============================] - 374s 312ms/step - loss: 1.1389 - auc_3: 0.6665 - val_loss: 1.1187 - val_auc_3: 0.7262\n",
      "\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 394s 318ms/step - loss: 1.1972 - auc_4: 0.6615 - val_loss: 1.0994 - val_auc_4: 0.7410\n",
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 380s 317ms/step - loss: 1.1409 - auc_4: 0.6666 - val_loss: 1.1304 - val_auc_4: 0.7105\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 378s 315ms/step - loss: 1.1202 - auc_4: 0.6731 - val_loss: 1.1535 - val_auc_4: 0.7231\n",
      "\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 391s 316ms/step - loss: 1.3265 - auc_5: 0.6726 - val_loss: 1.0451 - val_auc_5: 0.7603\n",
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 375s 312ms/step - loss: 1.1396 - auc_5: 0.6672 - val_loss: 1.0396 - val_auc_5: 0.7783\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 375s 313ms/step - loss: 1.1313 - auc_5: 0.6694 - val_loss: 1.1452 - val_auc_5: 0.7188\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 375s 312ms/step - loss: 1.1673 - auc_5: 0.6554 - val_loss: 1.0935 - val_auc_5: 0.7352\n",
      "\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 390s 313ms/step - loss: 1.2702 - auc_6: 0.6688 - val_loss: 1.1679 - val_auc_6: 0.6841\n",
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 373s 311ms/step - loss: 1.1681 - auc_6: 0.6526 - val_loss: 1.2054 - val_auc_6: 0.6363\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 379s 315ms/step - loss: 1.1548 - auc_6: 0.6540 - val_loss: 1.1257 - val_auc_6: 0.7326\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 379s 315ms/step - loss: 1.1530 - auc_6: 0.6605 - val_loss: 1.1415 - val_auc_6: 0.7180\n",
      "Epoch 5/15\n",
      "1200/1200 [==============================] - 378s 315ms/step - loss: 1.1177 - auc_6: 0.6714 - val_loss: 1.1746 - val_auc_6: 0.6872\n",
      "Model trained successfully with mean AUC score of \n",
      " Train_AUC: 0.66546\n",
      " Valid_AUC: 0.74532 \n",
      " Train_loss: 1.1474199999999999 \n",
      " valid_loss: 1.0912000000000002\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.Sequential([\n",
    "        pretrained_model.layers[0],\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Flatten()\n",
    "    ])\n",
    "\n",
    "print('****Model loaded successfully****\\n')\n",
    "\n",
    "models_history = train_model(new_model, 0.0001, 15, folds = 5, chkp_path = 'EfficientNet-B5/chexpert/pretrain_320px/320px', img_size = 320, channels = 1)    \n",
    "print(f'Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]} \\\n",
    "\\n Train_loss: {models_history[\"loss\"]} \\n valid_loss: {models_history[\"val_loss\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetV2-S 600px pretrained on 600px chexpert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the file names\n",
    "    \n",
    "batchSize = 8\n",
    "input_height, input_width = (600,600)\n",
    "train_dir = pathlib.Path(os.path.join(data_dir, f'{input_height}px','train','study'))\n",
    "train_image_paths = list(train_dir.glob('*.png'))\n",
    "train_image_paths = [path for path in train_image_paths if str(path).split(os.sep)[-1].split('.')[0] in list(df.StudyInstanceUID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pretrained_model = tf.keras.models.load_model('.\\saved_models\\efficientNetV2_s_600px.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# new_model = effnetv2_model.get_model('efficientnetv2-s', include_top = False, pretrained = False)\n",
    "# new_model.set_weights(pretrained_model.layers[0].get_weights())\n",
    "\n",
    "# print('****Model loaded successfully****\\n')\n",
    "\n",
    "# models_history = train_model(new_model, 0.0001, 20, folds = 5, chkp_path = 'EfficientNetV2-S/chexpert/320px')    \n",
    "# print(f'Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]} \\\n",
    "# \\n Train_loss: {models_history[\"loss\"]} \\n valid_loss: {models_history[\"val_loss\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetV2-S 512px pretrained on 512px chexpert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the file names\n",
    "    \n",
    "batchSize = 4\n",
    "input_height, input_width = (512,512)\n",
    "train_dir = pathlib.Path(os.path.join(data_dir, f'{input_height}px','train','study'))\n",
    "train_image_paths = list(train_dir.glob('*.png'))\n",
    "train_image_paths = [path for path in train_image_paths if str(path).split(os.sep)[-1].split('.')[0] in list(df.StudyInstanceUID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_59403586) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_59330846) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59395808) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_59330779) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_59404158) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_59337865) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_59334922) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_59339993) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_59402923) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_59331686) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_59402782) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_59332609) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_59330309) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_59339035) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_59339603) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_59379720) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_59329722) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_59333004) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_59331290) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_59404444) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_59334588) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_59403872) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_59402314) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_59406142) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_59405570) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_59403300) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_59406606) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_59338255) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_59336333) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_59333928) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_59410020) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_59405856) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_59401488) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_59402270) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_59411272) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_59404998) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_59405462) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_59401864) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_59333532) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_59402176) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59362203) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_59342889) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59354478) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_59338450) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_59410700) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_59402409) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_59339798) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_59407177) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_59401444) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_59410878) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_59331949) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_59333268) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_59408984) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_59330579) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_59410128) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_59340383) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_59401959) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_59401289) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_59332873) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_59332213) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_59408126) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_59334060) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_59342346) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_59407554) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_59341731) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_59403014) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_59334192) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_59331817) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_59401664) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_59342777) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_59342458) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_59401590) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59392177) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_59337670) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_59332081) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_59408018) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_59401386) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_59336695) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_59408876) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_59341536) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_59342104) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_59330512) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_59404890) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_59330389) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_59405748) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_59404621) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_59407446) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_59340773) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_59409556) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_59410414) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_59409162) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_59408698) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_59341146) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_59401262) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_59339408) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_59337280) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_59407000) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_59337085) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_59403192) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_59410986) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_59403478) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_59401349) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_59402020) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_59333136) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_59339213) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_59404712) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_59402626) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_59334852) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_59403764) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_59408412) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_59343202) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_59337475) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_59410592) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_59404336) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_59410306) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_59330646) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_59406320) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_59338645) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_59402115) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59389113) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_59333796) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_59409448) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_59406034) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_59341926) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_59331158) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_59331422) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_59405284) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_59408304) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_59401546) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_59409734) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_59332477) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_59376074) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_59340188) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_59333400) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_59342553) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_59333664) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_59401708) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_59343069) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_59330712) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_59342665) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_59402565) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_59332345) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_59338840) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_59331026) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_59340951) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_59411164) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_59409270) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_59334456) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_59404050) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_59407732) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_59406428) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_59330349) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_59401803) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_59330445) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_59343144) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_59334324) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_59342984) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_59330913) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59385482) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_59336890) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_59334720) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_59409842) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_59406892) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_59341341) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_59408590) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_59338060) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_59332741) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_59407268) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_59342234) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_59340578) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_59405176) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_59402721) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_59406714) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_59402470) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_59407840) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_59336500) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_59331554) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = tf.keras.models.load_model(r'.\\checkpoints\\train\\efficientNetV2-s\\512px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetv2-s (EffNetV2Mo (None, 1280)              20331360  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 5124      \n",
      "=================================================================\n",
      "Total params: 20,336,484\n",
      "Trainable params: 20,182,612\n",
      "Non-trainable params: 153,872\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 367s 291ms/step - loss: 1.1032 - auc: 0.7807 - val_loss: 0.9854 - val_auc: 0.8317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, tpu_batch_normalization_layer_call_and_return_conditional_losses, tpu_batch_normalization_layer_call_fn, conv2d_layer_call_and_return_conditional_losses while saving (showing 5 of 1405). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 349s 291ms/step - loss: 0.9747 - auc: 0.8305 - val_loss: 1.0041 - val_auc: 0.8243\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 348s 290ms/step - loss: 0.8545 - auc: 0.8682 - val_loss: 1.0654 - val_auc: 0.8088\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 371s 299ms/step - loss: 0.9727 - auc_1: 0.8337 - val_loss: 0.7705 - val_auc_1: 0.8942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, tpu_batch_normalization_layer_call_and_return_conditional_losses, tpu_batch_normalization_layer_call_fn, conv2d_layer_call_and_return_conditional_losses while saving (showing 5 of 1405). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 356s 296ms/step - loss: 0.7897 - auc_1: 0.8864 - val_loss: 0.8422 - val_auc_1: 0.8774\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 352s 293ms/step - loss: 0.6456 - auc_1: 0.9194 - val_loss: 0.8735 - val_auc_1: 0.8813\n",
      "\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 361s 290ms/step - loss: 0.8396 - auc_2: 0.8774 - val_loss: 0.5683 - val_auc_2: 0.9419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, tpu_batch_normalization_layer_call_and_return_conditional_losses, tpu_batch_normalization_layer_call_fn, conv2d_layer_call_and_return_conditional_losses while saving (showing 5 of 1405). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 346s 288ms/step - loss: 0.6414 - auc_2: 0.9189 - val_loss: 0.5247 - val_auc_2: 0.9461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, tpu_batch_normalization_layer_call_and_return_conditional_losses, tpu_batch_normalization_layer_call_fn, conv2d_layer_call_and_return_conditional_losses while saving (showing 5 of 1405). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 346s 288ms/step - loss: 0.5154 - auc_2: 0.9421 - val_loss: 0.6203 - val_auc_2: 0.9319\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 346s 288ms/step - loss: 0.4851 - auc_2: 0.9473 - val_loss: 0.5678 - val_auc_2: 0.9417\n",
      "\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 359s 289ms/step - loss: 0.6939 - auc_3: 0.9150 - val_loss: 0.3775 - val_auc_3: 0.9742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, tpu_batch_normalization_layer_call_and_return_conditional_losses, tpu_batch_normalization_layer_call_fn, conv2d_layer_call_and_return_conditional_losses while saving (showing 5 of 1405). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 346s 288ms/step - loss: 0.5130 - auc_3: 0.9455 - val_loss: 0.3908 - val_auc_3: 0.9724\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 345s 287ms/step - loss: 0.4001 - auc_3: 0.9630 - val_loss: 0.6626 - val_auc_3: 0.9291\n",
      "\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 362s 289ms/step - loss: 0.6218 - auc_4: 0.9323 - val_loss: 0.2688 - val_auc_4: 0.9892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, tpu_batch_normalization_layer_call_and_return_conditional_losses, tpu_batch_normalization_layer_call_fn, conv2d_layer_call_and_return_conditional_losses while saving (showing 5 of 1405). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 347s 289ms/step - loss: 0.4339 - auc_4: 0.9602 - val_loss: 0.2647 - val_auc_4: 0.9873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, tpu_batch_normalization_layer_call_and_return_conditional_losses, tpu_batch_normalization_layer_call_fn, conv2d_layer_call_and_return_conditional_losses while saving (showing 5 of 1405). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 351s 292ms/step - loss: 0.3376 - auc_4: 0.9722 - val_loss: 0.3313 - val_auc_4: 0.9791\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 352s 293ms/step - loss: 0.3108 - auc_4: 0.9747 - val_loss: 0.3065 - val_auc_4: 0.9819\n",
      "Model trained successfully with mean AUC score of \n",
      " Train_AUC: 0.8817\n",
      " Valid_AUC: 0.9267000000000001 \n",
      " Train_loss: 0.76902 \n",
      " valid_loss: 0.58456\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "new_model = effnetv2_model.get_model('efficientnetv2-s', include_top = False, pretrained = False)\n",
    "new_model.set_weights(pretrained_model.layers[0].get_weights())\n",
    "\n",
    "print('****Model loaded successfully****\\n')\n",
    "\n",
    "models_history = train_model(new_model, 0.0001, 15, folds = 5, chkp_path = 'EfficientNetV2-S/chexpert/pretrain_512px/512px', img_size = 512, channels = 3)    \n",
    "print(f'Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]} \\\n",
    "\\n Train_loss: {models_history[\"loss\"]} \\n valid_loss: {models_history[\"val_loss\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained successfully with mean AUC score of <br>\n",
    " Train_AUC: 0.87748<br>\n",
    " Valid_AUC: 0.9214 <br>\n",
    " Train_loss: 0.7798 <br>\n",
    " valid_loss: 0.61614"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetV2-S 320px pretrained on 512px chexpert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the file names\n",
    "    \n",
    "batchSize = 4\n",
    "input_height, input_width = (320,320)\n",
    "train_dir = pathlib.Path(os.path.join(data_dir, f'{input_height}px','train','study'))\n",
    "train_image_paths = list(train_dir.glob('*.png'))\n",
    "train_image_paths = [path for path in train_image_paths if str(path).split(os.sep)[-1].split('.')[0] in list(df.StudyInstanceUID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_59403586) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_59330846) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59395808) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_59330779) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_59404158) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_59337865) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_59334922) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_59339993) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_59402923) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_59331686) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_59402782) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_59332609) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_59330309) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_59339035) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_59339603) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_59379720) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_59329722) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_59333004) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_59331290) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_59404444) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_59334588) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_59403872) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_59402314) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_59406142) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_59405570) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_59403300) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_59406606) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_59338255) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_59336333) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_59333928) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_59410020) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_59405856) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_59401488) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_59402270) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_59411272) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_59404998) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_59405462) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_59401864) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_59333532) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_59402176) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59362203) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_59342889) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59354478) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_59338450) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_59410700) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_59402409) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_59339798) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_59407177) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_59401444) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_59410878) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_59331949) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_59333268) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_59408984) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_59330579) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_59410128) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_59340383) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_59401959) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_59401289) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_59332873) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_59332213) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_59408126) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_59334060) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_59342346) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_59407554) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_59341731) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_59403014) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_59334192) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_59331817) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_59401664) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_59342777) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_59342458) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_59401590) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59392177) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_59337670) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_59332081) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_59408018) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_59401386) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_59336695) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_59408876) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_59341536) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_59342104) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_59330512) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_59404890) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_59330389) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_59405748) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_59404621) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_59407446) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_59340773) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_59409556) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_59410414) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_59409162) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_59408698) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_59341146) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_59401262) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_59339408) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_59337280) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_59407000) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_59337085) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_59403192) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_59410986) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_59403478) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_59401349) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_59402020) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_59333136) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_59339213) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_59404712) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_59402626) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_59334852) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_59403764) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_59408412) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_59343202) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_59337475) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_59410592) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_59404336) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_59410306) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_59330646) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_59406320) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_59338645) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_59402115) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59389113) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_59333796) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_59409448) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_59406034) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_59341926) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_59331158) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_59331422) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_59405284) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_59408304) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_59401546) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_59409734) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_59332477) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_59376074) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_59340188) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_59333400) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_59342553) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_59333664) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_59401708) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_59343069) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_59330712) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_59342665) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_59402565) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_59332345) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_59338840) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_59331026) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_59340951) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_59411164) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_59409270) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_59334456) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_59404050) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_59407732) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_59406428) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_59330349) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_59401803) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_59330445) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_59343144) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_59334324) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_59342984) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_59330913) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_59385482) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_59336890) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_59334720) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_59409842) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_59406892) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_59341341) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_59408590) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_59338060) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_59332741) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_59407268) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_59342234) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_59340578) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_59405176) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_59402721) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_59406714) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_59402470) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_59407840) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_59336500) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_59331554) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = tf.keras.models.load_model(r'.\\checkpoints\\train\\efficientNetV2-s\\512px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetv2-s (EffNetV2Mo (None, 1280)              20331360  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 5124      \n",
      "=================================================================\n",
      "Total params: 20,336,484\n",
      "Trainable params: 20,182,612\n",
      "Non-trainable params: 153,872\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 370s 298ms/step - loss: 1.1069 - auc: 0.7780 - val_loss: 0.9722 - val_auc: 0.8362\n",
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 350s 291ms/step - loss: 0.9725 - auc: 0.8304 - val_loss: 1.0104 - val_auc: 0.8197\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 349s 291ms/step - loss: 0.8514 - auc: 0.8685 - val_loss: 1.0943 - val_auc: 0.8010\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 349s 291ms/step - loss: 0.7701 - auc: 0.8890 - val_loss: 1.1677 - val_auc: 0.7929\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 364s 294ms/step - loss: 0.9378 - auc_1: 0.8454 - val_loss: 0.6832 - val_auc_1: 0.9166\n",
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 354s 295ms/step - loss: 0.7379 - auc_1: 0.8981 - val_loss: 0.6784 - val_auc_1: 0.9166\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 357s 297ms/step - loss: 0.5748 - auc_1: 0.9336 - val_loss: 0.7540 - val_auc_1: 0.9072\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 353s 294ms/step - loss: 0.5403 - auc_1: 0.9400 - val_loss: 0.7277 - val_auc_1: 0.9123\n",
      "Epoch 5/15\n",
      "1200/1200 [==============================] - 356s 296ms/step - loss: 0.3825 - auc_1: 0.9680 - val_loss: 0.7021 - val_auc_1: 0.9197\n",
      "\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 369s 297ms/step - loss: 0.7294 - auc_2: 0.9083 - val_loss: 0.4428 - val_auc_2: 0.9640\n",
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 350s 291ms/step - loss: 0.5290 - auc_2: 0.9437 - val_loss: 0.4238 - val_auc_2: 0.9669\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 349s 291ms/step - loss: 0.4110 - auc_2: 0.9632 - val_loss: 0.6565 - val_auc_2: 0.9372\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 352s 294ms/step - loss: 0.3773 - auc_2: 0.9669 - val_loss: 0.4748 - val_auc_2: 0.9609\n",
      "Epoch 5/15\n",
      "1200/1200 [==============================] - 356s 296ms/step - loss: 0.2666 - auc_2: 0.9798 - val_loss: 0.4261 - val_auc_2: 0.9663\n",
      "\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 371s 300ms/step - loss: 0.6127 - auc_3: 0.9347 - val_loss: 0.3418 - val_auc_3: 0.9800\n",
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 354s 295ms/step - loss: 0.4168 - auc_3: 0.9620 - val_loss: 0.3508 - val_auc_3: 0.9771\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 353s 294ms/step - loss: 0.3178 - auc_3: 0.9733 - val_loss: 0.3572 - val_auc_3: 0.9757\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 353s 294ms/step - loss: 0.3101 - auc_3: 0.9726 - val_loss: 0.2964 - val_auc_3: 0.9827\n",
      "Epoch 5/15\n",
      "1200/1200 [==============================] - 351s 292ms/step - loss: 0.2173 - auc_3: 0.9824 - val_loss: 0.2592 - val_auc_3: 0.9863\n",
      "Epoch 6/15\n",
      "1200/1200 [==============================] - 352s 293ms/step - loss: 0.2210 - auc_3: 0.9814 - val_loss: 0.5700 - val_auc_3: 0.9481\n",
      "Epoch 7/15\n",
      "1200/1200 [==============================] - 351s 293ms/step - loss: 0.2145 - auc_3: 0.9828 - val_loss: 0.3750 - val_auc_3: 0.9742\n",
      "Epoch 8/15\n",
      "1200/1200 [==============================] - 353s 294ms/step - loss: 0.1715 - auc_3: 0.9862 - val_loss: 0.3638 - val_auc_3: 0.9751\n",
      "\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "Epoch 1/15\n",
      "1200/1200 [==============================] - 370s 297ms/step - loss: 0.5192 - auc_4: 0.9546 - val_loss: 0.2216 - val_auc_4: 0.9930\n",
      "Epoch 2/15\n",
      "1200/1200 [==============================] - 355s 295ms/step - loss: 0.3164 - auc_4: 0.9769 - val_loss: 0.1757 - val_auc_4: 0.9946\n",
      "Epoch 3/15\n",
      "1200/1200 [==============================] - 357s 297ms/step - loss: 0.2141 - auc_4: 0.9859 - val_loss: 0.1553 - val_auc_4: 0.9948\n",
      "Epoch 4/15\n",
      "1200/1200 [==============================] - 359s 299ms/step - loss: 0.2137 - auc_4: 0.9841 - val_loss: 0.1500 - val_auc_4: 0.9939\n",
      "Epoch 5/15\n",
      "1200/1200 [==============================] - 355s 296ms/step - loss: 0.1366 - auc_4: 0.9920 - val_loss: 0.1141 - val_auc_4: 0.9965\n",
      "Epoch 6/15\n",
      "1200/1200 [==============================] - 358s 299ms/step - loss: 0.1459 - auc_4: 0.9911 - val_loss: 0.1988 - val_auc_4: 0.9912\n",
      "Epoch 7/15\n",
      "1200/1200 [==============================] - 358s 298ms/step - loss: 0.1402 - auc_4: 0.9914 - val_loss: 0.1914 - val_auc_4: 0.9900\n",
      "Epoch 8/15\n",
      "1200/1200 [==============================] - 356s 296ms/step - loss: 0.1156 - auc_4: 0.9935 - val_loss: 0.2490 - val_auc_4: 0.9847\n",
      "Model trained successfully with mean AUC score of \n",
      " Train_AUC: 0.91884\n",
      " Valid_AUC: 0.9404999999999999 \n",
      " Train_loss: 0.54554 \n",
      " valid_loss: 0.48954\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "new_model = effnetv2_model.get_model('efficientnetv2-s', include_top = False, pretrained = False)\n",
    "new_model.set_weights(pretrained_model.layers[0].get_weights())\n",
    "\n",
    "print('****Model loaded successfully****\\n')\n",
    "\n",
    "models_history = train_model(new_model, 0.0001, 15, folds = 5, chkp_path = 'EfficientNetV2-S/chexpert/pretrain_512px/320px')    \n",
    "print(f'Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]} \\\n",
    "\\n Train_loss: {models_history[\"loss\"]} \\n valid_loss: {models_history[\"val_loss\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetV2-S 320px pretrained on 320px chexpert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the file names\n",
    "    \n",
    "batchSize = 8\n",
    "input_height, input_width = (320,320)\n",
    "train_dir = pathlib.Path(os.path.join(data_dir, f'{input_height}px','train','study'))\n",
    "train_image_paths = list(train_dir.glob('*.png'))\n",
    "train_image_paths = [path for path in train_image_paths if str(path).split(os.sep)[-1].split('.')[0] in list(df.StudyInstanceUID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_74803737) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_74799574) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_74805150) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_74728641) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_74727339) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_74727272) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_74807546) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_74798268) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_74806688) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_74736748) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_74739449) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_74730224) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_74776280) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_74798048) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_74772634) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_74807832) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_74807724) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_74797849) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_74806116) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_74802988) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_74728377) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_74805436) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_74807152) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_74802022) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_74801181) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_74733645) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_74731482) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_74726909) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_74801736) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_74732893) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_74735205) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_74729960) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_74728509) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_74729696) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_74800146) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_74734035) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_74729828) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_74798830) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_74735968) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_74727718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_74799752) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_74728246) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_74727473) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_74727139) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_74806580) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_74799186) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_74798363) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_74804578) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_74733060) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_74798150) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74782042) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_74797822) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_74726949) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_74735400) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_74734230) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_74737511) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_74807260) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_74800038) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_74801558) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_74727005) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74788737) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_74803560) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74758763) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_74735010) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_74804686) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74751038) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_74735773) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_74800432) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_74730092) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_74799281) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_74798106) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_74736943) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_74800610) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_74799342) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_74801450) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_74739629) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_74806294) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_74728114) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_74799125) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_74738794) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_74726869) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_74802130) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_74737901) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_74730752) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_74800718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_74801844) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_74798874) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_74800324) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_74733255) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_74800896) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_74802880) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_74729037) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_74798736) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_74798519) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_74807438) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_74797909) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_74737333) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_74802308) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_74801004) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_74739337) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_74734620) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_74804864) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_74729433) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_74802702) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_74738664) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_74805830) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_74804972) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_74727072) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_74730356) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_74739544) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_74731412) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74792368) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_74729169) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_74738906) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_74731148) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_74799030) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_74806974) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_74733840) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_74734425) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_74804006) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_74803166) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_74739018) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_74798580) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_74739762) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_74798004) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_74798675) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_74735595) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74785673) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_74799860) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_74799483) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_74802594) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_74738096) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_74737138) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_74805722) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_74801272) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_74730884) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_74803274) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_74802416) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_74728905) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_74737706) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_74728773) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_74806866) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_74731016) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_74736553) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_74806402) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_74804400) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_74727406) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_74738291) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_74730488) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_74727586) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_74734815) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_74727850) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_74798224) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_74804292) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_74730620) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_74726282) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_74733450) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_74798969) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_74798424) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_74727982) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_74739704) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_74739113) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_74803452) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_74797946) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_74736358) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_74729301) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_74738486) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_74806008) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_74803828) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_74731280) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_74729564) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_74805544) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_74805258) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_74736163) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_74739225) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_74727206) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_74804114) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = tf.keras.models.load_model(r'.\\checkpoints\\train\\efficientNetV2-s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetv2-s (EffNetV2Mo (None, 1280)              20331360  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 5124      \n",
      "=================================================================\n",
      "Total params: 20,336,484\n",
      "Trainable params: 20,182,612\n",
      "Non-trainable params: 153,872\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 171s 266ms/step - loss: 1.1216 - auc: 0.7730 - val_loss: 0.9965 - val_auc: 0.8255\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.9944 - auc: 0.8258 - val_loss: 1.0248 - val_auc: 0.8152\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.8613 - auc: 0.8706 - val_loss: 1.0901 - val_auc: 0.8003\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.7584 - auc: 0.9011 - val_loss: 1.2173 - val_auc: 0.7950\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 170s 263ms/step - loss: 0.9226 - auc_1: 0.8543 - val_loss: 0.6131 - val_auc_1: 0.9334\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.7135 - auc_1: 0.9115 - val_loss: 0.6296 - val_auc_1: 0.9310\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 155s 258ms/step - loss: 0.5695 - auc_1: 0.9432 - val_loss: 0.8407 - val_auc_1: 0.8964\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 156s 259ms/step - loss: 0.5053 - auc_1: 0.9535 - val_loss: 0.6923 - val_auc_1: 0.9243\n",
      "\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 171s 265ms/step - loss: 0.7157 - auc_2: 0.9135 - val_loss: 0.3745 - val_auc_2: 0.9773\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.5139 - auc_2: 0.9518 - val_loss: 0.3757 - val_auc_2: 0.9731\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.3896 - auc_2: 0.9712 - val_loss: 0.4789 - val_auc_2: 0.9596\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.3426 - auc_2: 0.9754 - val_loss: 0.3801 - val_auc_2: 0.9721\n",
      "\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 172s 264ms/step - loss: 0.5849 - auc_3: 0.9467 - val_loss: 0.1735 - val_auc_3: 0.9955\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.3936 - auc_3: 0.9706 - val_loss: 0.1900 - val_auc_3: 0.9939\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.2918 - auc_3: 0.9825 - val_loss: 0.2433 - val_auc_3: 0.9872\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.2777 - auc_3: 0.9825 - val_loss: 0.2053 - val_auc_3: 0.9912\n",
      "\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 172s 267ms/step - loss: 0.4994 - auc_4: 0.9638 - val_loss: 0.1183 - val_auc_4: 0.9984\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.2911 - auc_4: 0.9832 - val_loss: 0.1164 - val_auc_4: 0.9971\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 158s 263ms/step - loss: 0.2167 - auc_4: 0.9893 - val_loss: 0.1962 - val_auc_4: 0.9911\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 1594s 3s/step - loss: 0.2189 - auc_4: 0.9884 - val_loss: 0.1254 - val_auc_4: 0.9960\n",
      "Epoch 5/15\n",
      "564/600 [===========================>..] - ETA: 3:02 - loss: 0.1238 - auc_4: 0.9955"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "new_model = effnetv2_model.get_model('efficientnetv2-s', include_top = False, pretrained = False)\n",
    "new_model.set_weights(pretrained_model.layers[0].get_weights())\n",
    "\n",
    "print('****Model loaded successfully****\\n')\n",
    "\n",
    "models_history = train_model(new_model, 0.0001, 15, folds = 5, chkp_path = 'EfficientNetV2-S/chexpert/320px')    \n",
    "print(f'Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]} \\\n",
    "\\n Train_loss: {models_history[\"loss\"]} \\n valid_loss: {models_history[\"val_loss\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained successfully with mean AUC score of \n",
    " Train_AUC: 0.9133000000000001\n",
    " Valid_AUC: 0.94634 \n",
    " Train_loss: 0.5715 \n",
    " valid_loss: 0.42942\n",
    " \n",
    " Note: The model needs to be trained again as it couldn't be saved this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetV2-S 320px pretrained on 320px chexpert dataset after data augmentation|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "random_flip (RandomFlip)     (None, 320, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "efficientnetv2-s (EffNetV2Mo (None, 1280)              20331360  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 5124      \n",
      "=================================================================\n",
      "Total params: 20,336,484\n",
      "Trainable params: 20,182,612\n",
      "Non-trainable params: 153,872\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 173s 267ms/step - loss: 1.1417 - auc: 0.7614 - val_loss: 1.0178 - val_auc: 0.8160\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 156s 261ms/step - loss: 1.0310 - auc: 0.8089 - val_loss: 1.0008 - val_auc: 0.8238\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 154s 257ms/step - loss: 0.9719 - auc: 0.8326 - val_loss: 1.0640 - val_auc: 0.8034\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 155s 258ms/step - loss: 0.9270 - auc: 0.8480 - val_loss: 1.0976 - val_auc: 0.8059\n",
      "Epoch 5/15\n",
      "600/600 [==============================] - 155s 258ms/step - loss: 0.8412 - auc: 0.8755 - val_loss: 1.0488 - val_auc: 0.8265\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 198s 310ms/step - loss: 1.0059 - auc_1: 0.8229 - val_loss: 0.7893 - val_auc_1: 0.8883\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 160s 266ms/step - loss: 0.9044 - auc_1: 0.8570 - val_loss: 0.8050 - val_auc_1: 0.8865\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.8287 - auc_1: 0.8819 - val_loss: 0.8827 - val_auc_1: 0.8667\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.7760 - auc_1: 0.8956 - val_loss: 0.8449 - val_auc_1: 0.8843\n",
      "\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 706s 1s/step - loss: 0.8991 - auc_2: 0.8631 - val_loss: 0.6544 - val_auc_2: 0.9258\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 156s 259ms/step - loss: 0.7825 - auc_2: 0.8931 - val_loss: 0.6290 - val_auc_2: 0.9311\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 157s 262ms/step - loss: 0.6653 - auc_2: 0.9226 - val_loss: 0.7201 - val_auc_2: 0.9143\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.6493 - auc_2: 0.9253 - val_loss: 0.6412 - val_auc_2: 0.9298\n",
      "Epoch 5/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.4992 - auc_2: 0.9547 - val_loss: 0.6137 - val_auc_2: 0.9364\n",
      "Epoch 6/15\n",
      "600/600 [==============================] - 157s 262ms/step - loss: 0.5133 - auc_2: 0.9513 - val_loss: 0.7608 - val_auc_2: 0.9150\n",
      "Epoch 7/15\n",
      "600/600 [==============================] - 156s 259ms/step - loss: 0.4684 - auc_2: 0.9573 - val_loss: 0.7608 - val_auc_2: 0.9154\n",
      "Epoch 8/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.3926 - auc_2: 0.9687 - val_loss: 0.7414 - val_auc_2: 0.9153\n",
      "\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 173s 265ms/step - loss: 0.7536 - auc_3: 0.9067 - val_loss: 0.3193 - val_auc_3: 0.9841\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 155s 259ms/step - loss: 0.5764 - auc_3: 0.9399 - val_loss: 0.3218 - val_auc_3: 0.9807\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 158s 263ms/step - loss: 0.4888 - auc_3: 0.9560 - val_loss: 0.3925 - val_auc_3: 0.9705\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.4449 - auc_3: 0.9613 - val_loss: 0.3467 - val_auc_3: 0.9767\n",
      "\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 171s 264ms/step - loss: 0.6725 - auc_4: 0.9313 - val_loss: 0.2660 - val_auc_4: 0.9894\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.4572 - auc_4: 0.9614 - val_loss: 0.2259 - val_auc_4: 0.9907\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.3954 - auc_4: 0.9702 - val_loss: 0.3492 - val_auc_4: 0.9771\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 159s 265ms/step - loss: 0.3905 - auc_4: 0.9700 - val_loss: 0.2419 - val_auc_4: 0.9887\n",
      "Epoch 5/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.2877 - auc_4: 0.9827 - val_loss: 0.1990 - val_auc_4: 0.9916\n",
      "Epoch 6/15\n",
      "600/600 [==============================] - 155s 259ms/step - loss: 0.2836 - auc_4: 0.9814 - val_loss: 0.3475 - val_auc_4: 0.9754\n",
      "Epoch 7/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.2921 - auc_4: 0.9802 - val_loss: 0.3070 - val_auc_4: 0.9808\n",
      "Epoch 8/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.2366 - auc_4: 0.9854 - val_loss: 0.2989 - val_auc_4: 0.9809\n",
      "Model trained successfully with mean AUC score of \n",
      " Train_AUC: 0.8951800000000001\n",
      " Valid_AUC: 0.92484 \n",
      " Train_loss: 0.7154800000000001 \n",
      " valid_loss: 0.5844199999999999\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "new_model = effnetv2_model.get_model('efficientnetv2-s', include_top = False, pretrained = False)\n",
    "new_model.set_weights(pretrained_model.layers[0].get_weights())\n",
    "\n",
    "print('****Model loaded successfully****\\n')\n",
    "\n",
    "models_history = train_model(new_model, 0.0001, 15, folds = 5, data_aug = True, chkp_path = 'EfficientNetV2-S/chexpert/320px_with_aug')    \n",
    "print(f'Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]} \\\n",
    "\\n Train_loss: {models_history[\"loss\"]} \\n valid_loss: {models_history[\"val_loss\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained successfully with mean AUC score of <br>\n",
    " Train_AUC: 0.8951<br>\n",
    " Valid_AUC: 0.9248 <br>\n",
    " Train_loss: 0.7154<br> \n",
    " valid_loss: 0.5844<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetV2-S 320px pretrained on 320px chexpert dataset after applying gamma correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the file names\n",
    "    \n",
    "batchSize = 8\n",
    "input_height, input_width = (320,320)\n",
    "train_dir = pathlib.Path(os.path.join(data_dir, f'{input_height}px','train','study'))\n",
    "train_image_paths = list(train_dir.glob('*.png'))\n",
    "train_image_paths = [path for path in train_image_paths if str(path).split(os.sep)[-1].split('.')[0] in list(df.StudyInstanceUID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_74803737) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_74799574) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_74805150) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_74728641) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_74727339) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_74727272) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_74807546) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_74798268) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_74806688) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_74736748) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_74739449) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_74730224) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_74776280) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_74798048) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_74772634) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_74807832) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_74807724) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_74797849) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_74806116) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_74802988) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_74728377) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_74805436) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_74807152) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_74802022) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_74801181) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_74733645) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_74731482) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_74726909) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_74801736) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_74732893) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_74735205) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_74729960) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_74728509) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_74729696) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_74800146) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_74734035) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_74729828) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_74798830) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_74735968) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_74727718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_74799752) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_74728246) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_74727473) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_74727139) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_74806580) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_74799186) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_74798363) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_74804578) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_74733060) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_74798150) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74782042) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_74797822) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_74726949) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_74735400) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_74734230) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_74737511) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_74807260) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_74800038) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_74801558) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_74727005) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74788737) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_74803560) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74758763) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_74735010) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_28_layer_call_and_return_conditional_losses_74804686) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74751038) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_74735773) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_74800432) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_74730092) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_74799281) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_74798106) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_74736943) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_74800610) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_74799342) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_74801450) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_1_layer_call_and_return_conditional_losses_74739629) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_74806294) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_74728114) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_74799125) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_9_layer_call_and_return_conditional_losses_74738794) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_74726869) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_74802130) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_74737901) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_74730752) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_14_layer_call_and_return_conditional_losses_74800718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_74801844) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_74798874) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_74800324) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_74733255) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_74800896) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_74802880) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_74729037) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_74798736) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_74798519) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_74807438) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_74797909) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_17_layer_call_and_return_conditional_losses_74737333) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_74802308) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_74801004) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_74739337) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_74734620) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_74804864) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_74729433) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_74802702) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_74738664) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_74805830) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_29_layer_call_and_return_conditional_losses_74804972) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_74727072) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_74730356) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_74739544) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_39_layer_call_and_return_conditional_losses_74731412) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74792368) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_74729169) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_74738906) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_74731148) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_74799030) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_74806974) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_74733840) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_74734425) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_74804006) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_74803166) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_74739018) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_4_layer_call_and_return_conditional_losses_74798580) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_layer_call_and_return_conditional_losses_74739762) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_74798004) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_74798675) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_74735595) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetv2-s_layer_call_and_return_conditional_losses_74785673) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_74799860) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_74799483) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_74802594) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_74738096) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_18_layer_call_and_return_conditional_losses_74737138) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_74805722) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_16_layer_call_and_return_conditional_losses_74801272) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_blocks_35_layer_call_and_return_conditional_losses_74730884) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_74803274) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_74802416) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_20_layer_call_and_return_conditional_losses_74728905) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_15_layer_call_and_return_conditional_losses_74737706) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_19_layer_call_and_return_conditional_losses_74728773) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_74806866) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_36_layer_call_and_return_conditional_losses_74731016) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_21_layer_call_and_return_conditional_losses_74736553) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_34_layer_call_and_return_conditional_losses_74806402) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_74804400) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_8_layer_call_and_return_conditional_losses_74727406) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_74738291) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_32_layer_call_and_return_conditional_losses_74730488) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_10_layer_call_and_return_conditional_losses_74727586) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_74734815) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_12_layer_call_and_return_conditional_losses_74727850) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_2_layer_call_and_return_conditional_losses_74798224) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_27_layer_call_and_return_conditional_losses_74804292) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_74730620) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_74726282) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_37_layer_call_and_return_conditional_losses_74733450) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_7_layer_call_and_return_conditional_losses_74798969) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_3_layer_call_and_return_conditional_losses_74798424) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_13_layer_call_and_return_conditional_losses_74727982) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_0_layer_call_and_return_conditional_losses_74739704) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_6_layer_call_and_return_conditional_losses_74739113) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_24_layer_call_and_return_conditional_losses_74803452) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_head_layer_call_and_return_conditional_losses_74797946) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_22_layer_call_and_return_conditional_losses_74736358) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_74729301) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_11_layer_call_and_return_conditional_losses_74738486) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_33_layer_call_and_return_conditional_losses_74806008) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_74803828) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_38_layer_call_and_return_conditional_losses_74731280) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_25_layer_call_and_return_conditional_losses_74729564) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_31_layer_call_and_return_conditional_losses_74805544) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_30_layer_call_and_return_conditional_losses_74805258) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_23_layer_call_and_return_conditional_losses_74736163) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_74739225) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_5_layer_call_and_return_conditional_losses_74727206) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_blocks_26_layer_call_and_return_conditional_losses_74804114) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = tf.keras.models.load_model(r'.\\checkpoints\\train\\efficientNetV2-s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetv2-s (EffNetV2Mo (None, 1280)              20331360  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 5124      \n",
      "=================================================================\n",
      "Total params: 20,336,484\n",
      "Trainable params: 20,182,612\n",
      "Non-trainable params: 153,872\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 175s 271ms/step - loss: 1.1221 - auc: 0.7736 - val_loss: 1.0223 - val_auc: 0.8147\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.9938 - auc: 0.8262 - val_loss: 1.0454 - val_auc: 0.8111\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 158s 263ms/step - loss: 0.8571 - auc: 0.8730 - val_loss: 1.1333 - val_auc: 0.7945\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 156s 259ms/step - loss: 0.7698 - auc: 0.8982 - val_loss: 1.2312 - val_auc: 0.7847\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 170s 263ms/step - loss: 0.9284 - auc_1: 0.8536 - val_loss: 0.6488 - val_auc_1: 0.9272\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.7413 - auc_1: 0.9050 - val_loss: 0.6899 - val_auc_1: 0.9195\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 163s 271ms/step - loss: 0.5605 - auc_1: 0.9456 - val_loss: 0.8233 - val_auc_1: 0.9031\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 164s 273ms/step - loss: 0.4899 - auc_1: 0.9555 - val_loss: 0.6986 - val_auc_1: 0.9238\n",
      "\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 180s 279ms/step - loss: 0.7392 - auc_2: 0.9090 - val_loss: 0.3681 - val_auc_2: 0.9781\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 164s 273ms/step - loss: 0.5159 - auc_2: 0.9521 - val_loss: 0.3981 - val_auc_2: 0.9707\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.3902 - auc_2: 0.9716 - val_loss: 0.4117 - val_auc_2: 0.9681\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 157s 261ms/step - loss: 0.3444 - auc_2: 0.9750 - val_loss: 0.4150 - val_auc_2: 0.9679\n",
      "\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 170s 262ms/step - loss: 0.6075 - auc_3: 0.9439 - val_loss: 0.1783 - val_auc_3: 0.9947\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 155s 258ms/step - loss: 0.3942 - auc_3: 0.9711 - val_loss: 0.1808 - val_auc_3: 0.9935\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 155s 258ms/step - loss: 0.2970 - auc_3: 0.9824 - val_loss: 0.2397 - val_auc_3: 0.9877\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 155s 258ms/step - loss: 0.2560 - auc_3: 0.9849 - val_loss: 0.1986 - val_auc_3: 0.9903\n",
      "\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 169s 263ms/step - loss: 0.5362 - auc_4: 0.9523 - val_loss: 0.1241 - val_auc_4: 0.9982\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 156s 259ms/step - loss: 0.3283 - auc_4: 0.9782 - val_loss: 0.0989 - val_auc_4: 0.9980\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 156s 259ms/step - loss: 0.2417 - auc_4: 0.9874 - val_loss: 0.1528 - val_auc_4: 0.9942\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.2225 - auc_4: 0.9878 - val_loss: 0.1029 - val_auc_4: 0.9970\n",
      "Epoch 5/15\n",
      "600/600 [==============================] - 156s 260ms/step - loss: 0.1381 - auc_4: 0.9945 - val_loss: 0.0810 - val_auc_4: 0.9984\n",
      "Epoch 6/15\n",
      "600/600 [==============================] - 155s 259ms/step - loss: 0.1424 - auc_4: 0.9938 - val_loss: 0.2033 - val_auc_4: 0.9902\n",
      "Epoch 7/15\n",
      "600/600 [==============================] - 155s 259ms/step - loss: 0.1662 - auc_4: 0.9915 - val_loss: 0.1385 - val_auc_4: 0.9939\n",
      "Epoch 8/15\n",
      "600/600 [==============================] - 156s 259ms/step - loss: 0.1189 - auc_4: 0.9946 - val_loss: 0.1521 - val_auc_4: 0.9921\n",
      "Model trained successfully with mean AUC score of \n",
      " Train_AUC: 0.8949200000000002\n",
      " Valid_AUC: 0.94262 \n",
      " Train_loss: 0.70706 \n",
      " valid_loss: 0.45970000000000005\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "new_model = effnetv2_model.get_model('efficientnetv2-s', include_top = False, pretrained = False)\n",
    "new_model.set_weights(pretrained_model.layers[0].get_weights())\n",
    "\n",
    "print('****Model loaded successfully****\\n')\n",
    "\n",
    "models_history = train_model(new_model, 0.0001, 15, folds = 5, chkp_path = 'EfficientNetV2-S/chexpert/320px_gamma')    \n",
    "print(f'Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]} \\\n",
    "\\n Train_loss: {models_history[\"loss\"]} \\n valid_loss: {models_history[\"val_loss\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Net B0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained on Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T17:01:30.326587Z",
     "iopub.status.busy": "2021-07-09T17:01:30.326208Z",
     "iopub.status.idle": "2021-07-09T18:28:22.885336Z",
     "shell.execute_reply": "2021-07-09T18:28:22.883598Z",
     "shell.execute_reply.started": "2021-07-09T17:01:30.326554Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 10, 10, 1280)      4049571   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 1280)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 128004    \n",
      "=================================================================\n",
      "Total params: 4,177,575\n",
      "Trainable params: 4,135,552\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n",
      "None\n",
      "*****Training Begins*****\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 104s 318ms/step - loss: 1.3607 - auc_4: 0.6700 - val_loss: 1.2415 - val_auc_4: 0.7140\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 94s 313ms/step - loss: 1.2390 - auc_4: 0.6942 - val_loss: 1.2548 - val_auc_4: 0.6979\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 94s 313ms/step - loss: 1.1508 - auc_4: 0.7395 - val_loss: 7.8970 - val_auc_4: 0.6650\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 94s 313ms/step - loss: 1.0293 - auc_4: 0.7760 - val_loss: nan - val_auc_4: 0.6584\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 10, 10, 1280)      4049571   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 1280)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 128004    \n",
      "=================================================================\n",
      "Total params: 4,177,575\n",
      "Trainable params: 4,135,552\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n",
      "None\n",
      "*****Training Begins*****\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 103s 314ms/step - loss: 1.2884 - auc_5: 0.7195 - val_loss: 1.2370 - val_auc_5: 0.7238\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 93s 310ms/step - loss: 1.1013 - auc_5: 0.7906 - val_loss: 1.1784 - val_auc_5: 0.7439\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 94s 312ms/step - loss: 0.9412 - auc_5: 0.8352 - val_loss: 1.3555 - val_auc_5: 0.6613\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 94s 314ms/step - loss: 0.8353 - auc_5: 0.8637 - val_loss: 1.4132 - val_auc_5: 0.6290\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 94s 313ms/step - loss: 0.6941 - auc_5: 0.8938 - val_loss: 1.1977 - val_auc_5: 0.7436\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 10, 10, 1280)      4049571   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 1280)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 128004    \n",
      "=================================================================\n",
      "Total params: 4,177,575\n",
      "Trainable params: 4,135,552\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n",
      "None\n",
      "*****Training Begins*****\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 103s 321ms/step - loss: 1.1606 - auc_6: 0.7823 - val_loss: 8.2730 - val_auc_6: 0.6180\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 95s 315ms/step - loss: 0.9473 - auc_6: 0.8395 - val_loss: 1.3090 - val_auc_6: 0.7433\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 95s 317ms/step - loss: 0.8163 - auc_6: 0.8681 - val_loss: nan - val_auc_6: 0.6109\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 95s 317ms/step - loss: 0.7043 - auc_6: 0.8900 - val_loss: 1.3948 - val_auc_6: 0.6884\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 95s 315ms/step - loss: 0.6113 - auc_6: 0.9099 - val_loss: 1.1934 - val_auc_6: 0.7798\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 95s 316ms/step - loss: 0.6014 - auc_6: 0.9079 - val_loss: 1.3449 - val_auc_6: 0.6946\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 95s 315ms/step - loss: 0.5084 - auc_6: 0.9205 - val_loss: 1.3417 - val_auc_6: 0.6220\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 98s 327ms/step - loss: 0.3868 - auc_6: 0.9318 - val_loss: 1.4215 - val_auc_6: 0.5918\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 10, 10, 1280)      4049571   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 1280)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 128004    \n",
      "=================================================================\n",
      "Total params: 4,177,575\n",
      "Trainable params: 4,135,552\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n",
      "None\n",
      "*****Training Begins*****\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 103s 320ms/step - loss: 1.0401 - auc_7: 0.8198 - val_loss: 1.2504 - val_auc_7: 0.7241\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 94s 314ms/step - loss: 0.7927 - auc_7: 0.8800 - val_loss: nan - val_auc_7: 0.5684\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 93s 311ms/step - loss: 0.6512 - auc_7: 0.9067 - val_loss: 1.7792 - val_auc_7: 0.6147\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 94s 314ms/step - loss: 0.5334 - auc_7: 0.9234 - val_loss: 1.4883 - val_auc_7: 0.4388\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 10, 10, 1280)      4049571   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 1280)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 128004    \n",
      "=================================================================\n",
      "Total params: 4,177,575\n",
      "Trainable params: 4,135,552\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n",
      "None\n",
      "*****Training Begins*****\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 104s 322ms/step - loss: 0.9185 - auc_8: 0.8528 - val_loss: 1.5504 - val_auc_8: 0.5211\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 93s 310ms/step - loss: 0.7090 - auc_8: 0.9005 - val_loss: 2.3046 - val_auc_8: 0.6242\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 98s 325ms/step - loss: 0.5591 - auc_8: 0.9241 - val_loss: nan - val_auc_8: 0.6726\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 100s 332ms/step - loss: 0.4542 - auc_8: 0.9351 - val_loss: nan - val_auc_8: 0.6253\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-99862f39f161>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'****Model loaded successfully****\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmodels_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchkp_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'EfficientNetB0/imagenet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'****Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]}****'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-101-b8446d4a087f>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(baseline_model, max_lr, epochs, folds, chkp_path, dropout_rate, label_smoothing)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# Get the aggregate auc and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mbest_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_best_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_folds_best_stats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbest_stats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-dce4b6646505>\u001b[0m in \u001b[0;36mget_best_stats\u001b[1;34m(all_folds_best_stats)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_best_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_folds_best_stats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbest_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mfold_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_folds_best_stats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfold_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbest_stats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    batchSize = 16\n",
    "    input_height, input_width = (320, 320)\n",
    "    baseline_model = tf.keras.applications.EfficientNetB0(include_top = False, weights = 'imagenet', input_shape = (input_height, input_width, 3))\n",
    "    model = tf.keras.models.Sequential([\n",
    "     baseline_model,\n",
    "     tf.keras.layers.MaxPool2D(),\n",
    "     tf.keras.layers.Flatten(),\n",
    "    ])\n",
    "    print('****Model loaded successfully****\\n')\n",
    "\n",
    "    models_history = train_model(model, max_lr = 0.0001, epochs = 20, folds = 5, dropout_rate= 0.5, chkp_path = 'EfficientNetB0/imagenet')    \n",
    "    print(f'****Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]}****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained on Chexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_153945128) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_153977703) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_153976047) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_153978212) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_153975494) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_153946520) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_153943937) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_153944112) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_153965765) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_153968643) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_153976644) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_153946353) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_153975355) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_153944668) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_153970752) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_153977659) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_153945322) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_153978073) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_153944954) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_153978256) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_153946311) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_153978809) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_153975538) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_153977150) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_153945280) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_153944147) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_153946179) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_153977520) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_153974849) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_153970402) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_153973420) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_153971764) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_153945357) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_153946102) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_153945893) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_153945935) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_153972867) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_153977106) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_153971625) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_153946597) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_153945776) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_153971258) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_153945741) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_153946388) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_153943513) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_153943708) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_153973237) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_153936663) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_153970796) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_153975032) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_153973743) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_153945163) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_153971119) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_153971302) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_153945698) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_153944264) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_153978626) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_153972823) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_153945566) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_153945489) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_153943743) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_153978765) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_153944710) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_153976600) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_153944306) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_153974479) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_153943471) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_153976967) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_153973926) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_153944341) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_153963989) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_153946562) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_153972317) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_153944919) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_153946144) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_153945531) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_153976461) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_153944069) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_153972684) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_153972178) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_153943902) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_153944473) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_153945970) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_153944745) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_153976091) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_153975908) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_153973882) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_153974988) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_153943665) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_153973376) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_153944516) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_153943860) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_153971808) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_153979132) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_153974435) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_153974296) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_153944877) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_153946714) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_153943548) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_153970613) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_153972361) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_153945086) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_153944551) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 10, 10, 1280)      4048991   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 1280)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 128004    \n",
      "=================================================================\n",
      "Total params: 4,176,995\n",
      "Trainable params: 4,134,976\n",
      "Non-trainable params: 42,019\n",
      "_________________________________________________________________\n",
      "None\n",
      "*****\n",
      "Training Begins*****\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 98s 304ms/step - loss: 1.4271 - auc_12: 0.7215 - val_loss: 1.3118 - val_auc_12: 0.7581\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 1.3408 - auc_12: 0.7523 - val_loss: 1.2719 - val_auc_12: 0.7796\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 92s 305ms/step - loss: 1.3003 - auc_12: 0.7730 - val_loss: 1.0835 - val_auc_12: 0.7910\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 1.1343 - auc_12: 0.7765 - val_loss: 1.1053 - val_auc_12: 0.7767\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 1.0920 - auc_12: 0.7942 - val_loss: 1.1036 - val_auc_12: 0.7781\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 89s 298ms/step - loss: 1.0906 - auc_12: 0.7939 - val_loss: 1.1105 - val_auc_12: 0.7781\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "*****\n",
      "Training Begins*****\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 99s 308ms/step - loss: 1.2090 - auc_13: 0.7397 - val_loss: 1.0395 - val_auc_13: 0.8016\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 1.1293 - auc_13: 0.7790 - val_loss: 1.0084 - val_auc_13: 0.8117\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 1.1020 - auc_13: 0.7940 - val_loss: 1.0229 - val_auc_13: 0.8030\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 1.0930 - auc_13: 0.8004 - val_loss: 1.0057 - val_auc_13: 0.8092\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 92s 305ms/step - loss: 1.0238 - auc_13: 0.8212 - val_loss: 1.0072 - val_auc_13: 0.8057\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 93s 309ms/step - loss: 0.9943 - auc_13: 0.7906 - val_loss: 1.0407 - val_auc_13: 0.7997\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 0.9247 - auc_13: 0.8057 - val_loss: 1.0418 - val_auc_13: 0.7868\n",
      "\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "*****\n",
      "Training Begins*****\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 103s 320ms/step - loss: 1.1709 - auc_14: 0.7488 - val_loss: 1.0791 - val_auc_14: 0.7976\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 1.0830 - auc_14: 0.7955 - val_loss: 1.0339 - val_auc_14: 0.8132\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 1.0134 - auc_14: 0.8171 - val_loss: 1.0471 - val_auc_14: 0.8087\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.9166 - auc_14: 0.8316 - val_loss: 1.0417 - val_auc_14: 0.8015\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.7873 - auc_14: 0.8614 - val_loss: 1.0013 - val_auc_14: 0.8170\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.8427 - auc_14: 0.8481 - val_loss: 1.0061 - val_auc_14: 0.8196\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.6989 - auc_14: 0.8688 - val_loss: 1.0712 - val_auc_14: 0.8018\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.6237 - auc_14: 0.8803 - val_loss: 1.1670 - val_auc_14: 0.7796\n",
      "\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "*****\n",
      "Training Begins*****\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 98s 304ms/step - loss: 1.1069 - auc_15: 0.7438 - val_loss: 0.8159 - val_auc_15: 0.8861\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.9054 - auc_15: 0.8344 - val_loss: 0.7517 - val_auc_15: 0.8925\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.7689 - auc_15: 0.8646 - val_loss: 0.7556 - val_auc_15: 0.8905\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 90s 300ms/step - loss: 0.6730 - auc_15: 0.8801 - val_loss: 0.7068 - val_auc_15: 0.8873\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.5799 - auc_15: 0.8983 - val_loss: 0.6378 - val_auc_15: 0.9001\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 0.6148 - auc_15: 0.8864 - val_loss: 0.6646 - val_auc_15: 0.8869\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 0.5526 - auc_15: 0.9000 - val_loss: 0.7137 - val_auc_15: 0.8717\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 0.4796 - auc_15: 0.9102 - val_loss: 0.6698 - val_auc_15: 0.8882\n",
      "\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "*****\n",
      "Training Begins*****\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 99s 306ms/step - loss: 0.9163 - auc_16: 0.7840 - val_loss: 0.5139 - val_auc_16: 0.9217\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 0.6374 - auc_16: 0.8665 - val_loss: 0.4635 - val_auc_16: 0.9057\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.5036 - auc_16: 0.8849 - val_loss: 0.4491 - val_auc_16: 0.9025\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.4465 - auc_16: 0.8949 - val_loss: 0.3950 - val_auc_16: 0.9268\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.3674 - auc_16: 0.9058 - val_loss: 0.3601 - val_auc_16: 0.9183\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 93s 309ms/step - loss: 0.4106 - auc_16: 0.8979 - val_loss: 0.4636 - val_auc_16: 0.8925\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 0.3820 - auc_16: 0.9033 - val_loss: 0.4616 - val_auc_16: 0.9121\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.3365 - auc_16: 0.9058 - val_loss: 0.4817 - val_auc_16: 0.9224\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'auc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-f475992414b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmodels_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchkp_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'EfficientNetB0/chexpert'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'****Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]}****'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'auc'"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    batchSize = 16\n",
    "    input_height, input_width = (320, 320)\n",
    "    pretrained_model = tf.keras.models.load_model('./saved_models/efficientNetB0/auc_0.7520/')\n",
    "    model = tf.keras.models.Sequential([\n",
    "        pretrained_model.layers[0],\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Flatten()\n",
    "    ])\n",
    "        \n",
    "    \n",
    "    print('****Model loaded successfully****\\n')\n",
    "\n",
    "    models_history = train_model(model, 0.0001, 20, folds = 5, chkp_path = 'EfficientNetB0/chexpert')    \n",
    "    print(f'****Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]}****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'****Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]}****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'loss': 0.8255800000000001,\n",
    " 'auc_': 0.84778,\n",
    " 'val_loss': 0.81768,\n",
    " 'val_auc_': 0.8471200000000001}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training from sratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 10, 10, 1280)      4048991   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 1280)        0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 128004    \n",
      "=================================================================\n",
      "Total params: 4,176,995\n",
      "Trainable params: 4,134,976\n",
      "Non-trainable params: 42,019\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 106s 330ms/step - loss: 1.3106 - auc_17: 0.6397 - val_loss: 1.2008 - val_auc_17: 0.7040\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 1.2778 - auc_17: 0.6469 - val_loss: 1.1963 - val_auc_17: 0.7251\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 1.2382 - auc_17: 0.6696 - val_loss: 1.1845 - val_auc_17: 0.7190\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 93s 309ms/step - loss: 1.2392 - auc_17: 0.6790 - val_loss: 1.1823 - val_auc_17: 0.7124\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 1.1906 - auc_17: 0.7075 - val_loss: 1.1819 - val_auc_17: 0.7223\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 1.2101 - auc_17: 0.6982 - val_loss: 1.2226 - val_auc_17: 0.7199\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 1.1892 - auc_17: 0.7021 - val_loss: 1.1780 - val_auc_17: 0.7173\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 1.1468 - auc_17: 0.7227 - val_loss: 1.1830 - val_auc_17: 0.7209\n",
      "Epoch 9/20\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 1.1127 - auc_17: 0.7358 - val_loss: 1.1775 - val_auc_17: 0.7259\n",
      "Epoch 10/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 1.0855 - auc_17: 0.7493 - val_loss: 1.1882 - val_auc_17: 0.7280\n",
      "Epoch 11/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 1.0877 - auc_17: 0.7475 - val_loss: 1.1869 - val_auc_17: 0.7236\n",
      "Epoch 12/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 1.0733 - auc_17: 0.7473 - val_loss: 1.2211 - val_auc_17: 0.7332\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 99s 305ms/step - loss: 1.2949 - auc_18: 0.6510 - val_loss: 1.2009 - val_auc_18: 0.6867\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 91s 301ms/step - loss: 1.2434 - auc_18: 0.6756 - val_loss: 1.1814 - val_auc_18: 0.7020\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 91s 301ms/step - loss: 1.2134 - auc_18: 0.6831 - val_loss: 1.1825 - val_auc_18: 0.6915\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 1.1812 - auc_18: 0.6949 - val_loss: 1.1677 - val_auc_18: 0.7085\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 1.1210 - auc_18: 0.7252 - val_loss: 1.1487 - val_auc_18: 0.7216\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 1.1415 - auc_18: 0.7189 - val_loss: 1.1569 - val_auc_18: 0.7042\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 1.0873 - auc_18: 0.7330 - val_loss: 1.1356 - val_auc_18: 0.7235\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 90s 300ms/step - loss: 1.0334 - auc_18: 0.7534 - val_loss: 1.1559 - val_auc_18: 0.7264\n",
      "Epoch 9/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 0.9588 - auc_18: 0.7832 - val_loss: 1.1519 - val_auc_18: 0.7297\n",
      "Epoch 10/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.8982 - auc_18: 0.7999 - val_loss: 1.1431 - val_auc_18: 0.7427\n",
      "\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 102s 317ms/step - loss: 1.2694 - auc_19: 0.6699 - val_loss: 1.2292 - val_auc_19: 0.6879\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 92s 305ms/step - loss: 1.2135 - auc_19: 0.6907 - val_loss: 1.1994 - val_auc_19: 0.7019\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 1.1664 - auc_19: 0.7011 - val_loss: 1.1828 - val_auc_19: 0.7054\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 92s 305ms/step - loss: 1.1175 - auc_19: 0.7196 - val_loss: 1.1570 - val_auc_19: 0.7234\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 1.0423 - auc_19: 0.7516 - val_loss: 1.1341 - val_auc_19: 0.7395\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 1.0562 - auc_19: 0.7552 - val_loss: 1.1536 - val_auc_19: 0.7389\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 1.0008 - auc_19: 0.7718 - val_loss: 1.1609 - val_auc_19: 0.7433\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 93s 309ms/step - loss: 0.9249 - auc_19: 0.7970 - val_loss: 1.1272 - val_auc_19: 0.7372\n",
      "Epoch 9/20\n",
      "300/300 [==============================] - 93s 310ms/step - loss: 0.8496 - auc_19: 0.8087 - val_loss: 1.1184 - val_auc_19: 0.7533\n",
      "Epoch 10/20\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 0.7870 - auc_19: 0.8231 - val_loss: 1.1226 - val_auc_19: 0.7546\n",
      "Epoch 11/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 0.7922 - auc_19: 0.8230 - val_loss: 1.1814 - val_auc_19: 0.7461\n",
      "Epoch 12/20\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 0.7732 - auc_19: 0.8272 - val_loss: 1.1352 - val_auc_19: 0.7422\n",
      "\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 99s 307ms/step - loss: 1.2611 - auc_20: 0.6816 - val_loss: 1.1338 - val_auc_20: 0.7225\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 91s 301ms/step - loss: 1.1689 - auc_20: 0.7176 - val_loss: 1.0909 - val_auc_20: 0.7562\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 92s 305ms/step - loss: 1.1036 - auc_20: 0.7311 - val_loss: 1.0326 - val_auc_20: 0.7761\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.9897 - auc_20: 0.7721 - val_loss: 1.0102 - val_auc_20: 0.7856\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 90s 300ms/step - loss: 0.9009 - auc_20: 0.8014 - val_loss: 0.9745 - val_auc_20: 0.7998\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 0.9075 - auc_20: 0.7967 - val_loss: 0.9757 - val_auc_20: 0.7954\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 0.8180 - auc_20: 0.8172 - val_loss: 1.0296 - val_auc_20: 0.7872\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 92s 305ms/step - loss: 0.7397 - auc_20: 0.8408 - val_loss: 0.9562 - val_auc_20: 0.8020\n",
      "Epoch 9/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.6611 - auc_20: 0.8534 - val_loss: 0.9804 - val_auc_20: 0.7952\n",
      "Epoch 10/20\n",
      "300/300 [==============================] - 93s 308ms/step - loss: 0.5825 - auc_20: 0.8720 - val_loss: 0.9526 - val_auc_20: 0.8051\n",
      "Epoch 11/20\n",
      "300/300 [==============================] - 92s 305ms/step - loss: 0.6432 - auc_20: 0.8584 - val_loss: 1.0256 - val_auc_20: 0.7808\n",
      "Epoch 12/20\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.5921 - auc_20: 0.8597 - val_loss: 1.0343 - val_auc_20: 0.7927\n",
      "Epoch 13/20\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.5752 - auc_20: 0.8670 - val_loss: 1.0155 - val_auc_20: 0.7927\n",
      "\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 105s 313ms/step - loss: 1.2184 - auc_21: 0.7009 - val_loss: 1.0828 - val_auc_21: 0.7494\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 1.0456 - auc_21: 0.7528 - val_loss: 0.9266 - val_auc_21: 0.8170\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 93s 309ms/step - loss: 0.9053 - auc_21: 0.8021 - val_loss: 0.8853 - val_auc_21: 0.8333\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 0.7907 - auc_21: 0.8255 - val_loss: 0.7711 - val_auc_21: 0.8618\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 93s 311ms/step - loss: 0.6910 - auc_21: 0.8519 - val_loss: 0.6627 - val_auc_21: 0.8799\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 92s 305ms/step - loss: 0.6843 - auc_21: 0.8473 - val_loss: 0.9949 - val_auc_21: 0.8221\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 93s 309ms/step - loss: 0.6101 - auc_21: 0.8581 - val_loss: 0.8338 - val_auc_21: 0.8574\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 0.5542 - auc_21: 0.8669 - val_loss: 0.6947 - val_auc_21: 0.8664\n",
      "****Model trained successfully with mean AUC score of \n",
      " Train_AUC: 4.001399999999999\n",
      " Valid_AUC: 3.8877****\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    batchSize = 16\n",
    "    input_height, input_width = (320, 320)\n",
    "    baseline_model = tf.keras.applications.EfficientNetB0(include_top = False, weights = None, input_shape = (input_height, input_width, 1))\n",
    "    model = tf.keras.models.Sequential([\n",
    "     baseline_model,\n",
    "     tf.keras.layers.MaxPool2D(),\n",
    "     tf.keras.layers.Flatten(),\n",
    "    ])\n",
    "        \n",
    "    \n",
    "    print('****Model loaded successfully****\\n')\n",
    "\n",
    "    models_history = train_model(model, 0.0001, 20, folds = 5, chkp_path = 'EfficientNetB0/no_weights')    \n",
    "    print(f'****Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]}****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77754"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "- EfficientNetB0 provided best results when pretrained on the chexpert dataset and worst when pretrained imagenet weights were used.\n",
    "- We got a mean validation accuray of 0.84 (approx.) with the chexpert dataset pretrained network and 0.77 when trained from scratch using only the competition data.\n",
    "- The results did not vary much as the training for the best model was very efficient and stable. Overfitting was very minimal. On the other hand when finetuned using imagenet weights overfitting was a major issue and even with lower learning rate the losses varied alot from epoch to epoch. \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying impact of image scales\n",
    "\n",
    "- Any resolution that is able to result in a validation AUC greater than 0.84 will be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetB0 using 512px sized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:07.684631Z",
     "iopub.status.busy": "2021-07-09T16:38:07.684244Z",
     "iopub.status.idle": "2021-07-09T16:38:08.035364Z",
     "shell.execute_reply": "2021-07-09T16:38:08.034205Z",
     "shell.execute_reply.started": "2021-07-09T16:38:07.684599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6054"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing all the file names\n",
    "train_dir = pathlib.Path(os.path.join(data_dir, '512px','train','study'))\n",
    "train_image_paths = list(train_dir.glob('*.png'))\n",
    "\n",
    "len(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:08.037735Z",
     "iopub.status.busy": "2021-07-09T16:38:08.037281Z",
     "iopub.status.idle": "2021-07-09T16:38:12.622283Z",
     "shell.execute_reply": "2021-07-09T16:38:12.621133Z",
     "shell.execute_reply.started": "2021-07-09T16:38:08.03767Z"
    }
   },
   "outputs": [],
   "source": [
    "train_image_paths = [path for path in train_image_paths if str(path).split(os.sep)[-1].split('.')[0] in list(df.StudyInstanceUID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 16, 16, 1280)      4048991   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 1280)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 81920)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 81920)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 327684    \n",
      "=================================================================\n",
      "Total params: 4,376,675\n",
      "Trainable params: 4,334,656\n",
      "Non-trainable params: 42,019\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 210s 338ms/step - loss: 1.3958 - auc_1: 0.6992 - val_loss: 1.2140 - val_auc_1: 0.7453\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 197s 328ms/step - loss: 1.1949 - auc_1: 0.6932 - val_loss: 1.0969 - val_auc_1: 0.7434\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 197s 328ms/step - loss: 1.1156 - auc_1: 0.7162 - val_loss: 1.1669 - val_auc_1: 0.7439\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 198s 330ms/step - loss: 1.0911 - auc_1: 0.7236 - val_loss: 1.0999 - val_auc_1: 0.7565\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 198s 330ms/step - loss: 1.0159 - auc_1: 0.7467 - val_loss: 1.1024 - val_auc_1: 0.7409\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 204s 329ms/step - loss: 1.2893 - auc_2: 0.7066 - val_loss: 1.0763 - val_auc_2: 0.7561\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 197s 328ms/step - loss: 1.1057 - auc_2: 0.7210 - val_loss: 1.0711 - val_auc_2: 0.7581\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 195s 325ms/step - loss: 1.0573 - auc_2: 0.7399 - val_loss: 1.1303 - val_auc_2: 0.7465\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 195s 324ms/step - loss: 1.0400 - auc_2: 0.7433 - val_loss: 1.0797 - val_auc_2: 0.7516\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 200s 332ms/step - loss: 0.9600 - auc_2: 0.7701 - val_loss: 1.0874 - val_auc_2: 0.7563\n",
      "\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 206s 332ms/step - loss: 1.3530 - auc_3: 0.7465 - val_loss: 1.0975 - val_auc_3: 0.7919\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 198s 331ms/step - loss: 1.1579 - auc_3: 0.7515 - val_loss: 1.0516 - val_auc_3: 0.7644\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 200s 333ms/step - loss: 1.0082 - auc_3: 0.7554 - val_loss: 1.0736 - val_auc_3: 0.7651\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 201s 334ms/step - loss: 0.9685 - auc_3: 0.7625 - val_loss: 1.0306 - val_auc_3: 0.7692\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 203s 338ms/step - loss: 0.8611 - auc_3: 0.7875 - val_loss: 1.1058 - val_auc_3: 0.7689\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 204s 340ms/step - loss: 0.8566 - auc_3: 0.7841 - val_loss: 1.0854 - val_auc_3: 0.7580\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 199s 332ms/step - loss: 0.7763 - auc_3: 0.7977 - val_loss: 1.1374 - val_auc_3: 0.7422\n",
      "\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 210s 338ms/step - loss: 1.1755 - auc_4: 0.7117 - val_loss: 0.9145 - val_auc_4: 0.8106\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 203s 338ms/step - loss: 0.9574 - auc_4: 0.7605 - val_loss: 0.8742 - val_auc_4: 0.8110\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 198s 330ms/step - loss: 0.8486 - auc_4: 0.7847 - val_loss: 0.8681 - val_auc_4: 0.8239\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 199s 332ms/step - loss: 0.7640 - auc_4: 0.7958 - val_loss: 0.8536 - val_auc_4: 0.8187\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 198s 330ms/step - loss: 0.6115 - auc_4: 0.8284 - val_loss: 0.8488 - val_auc_4: 0.8209\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 197s 328ms/step - loss: 0.6305 - auc_4: 0.8205 - val_loss: 0.9626 - val_auc_4: 0.7807\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 197s 328ms/step - loss: 0.6033 - auc_4: 0.8205 - val_loss: 0.9936 - val_auc_4: 0.7756\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 199s 331ms/step - loss: 0.5202 - auc_4: 0.8343 - val_loss: 0.9701 - val_auc_4: 0.7924\n",
      "\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 219s 338ms/step - loss: 1.1223 - auc_5: 0.7636 - val_loss: 0.7991 - val_auc_5: 0.8529\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 200s 333ms/step - loss: 0.7758 - auc_5: 0.8053 - val_loss: 0.7903 - val_auc_5: 0.8532\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 201s 335ms/step - loss: 0.6463 - auc_5: 0.8248 - val_loss: 0.7127 - val_auc_5: 0.8783\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 203s 338ms/step - loss: 0.6397 - auc_5: 0.8278 - val_loss: 0.6247 - val_auc_5: 0.8657\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 202s 336ms/step - loss: 0.5185 - auc_5: 0.8474 - val_loss: 0.6252 - val_auc_5: 0.8680\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 204s 339ms/step - loss: 0.5213 - auc_5: 0.8399 - val_loss: 0.7461 - val_auc_5: 0.8366\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 206s 342ms/step - loss: 0.5008 - auc_5: 0.8438 - val_loss: 0.6101 - val_auc_5: 0.8649\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 206s 343ms/step - loss: 0.4162 - auc_5: 0.8548 - val_loss: 0.6748 - val_auc_5: 0.8492\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 205s 342ms/step - loss: 0.3939 - auc_5: 0.8539 - val_loss: 0.7871 - val_auc_5: 0.8398\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 205s 342ms/step - loss: 0.3780 - auc_5: 0.8549 - val_loss: 0.6396 - val_auc_5: 0.8562\n",
      "********************\n",
      "Model trained successfully with mean AUC score of \n",
      " Train_AUC: 0.7697800000000001\n",
      " Valid_AUC: 0.7913\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "batchSize = 8\n",
    "input_height, input_width = (512,512)\n",
    "pretrained_model = tf.keras.models.load_model('./saved_models/efficientNetB0/auc_0.7520/')\n",
    "\n",
    "baseline_model = change_model_input_size(pretrained_model, input_height, input_width) \n",
    "model = tf.keras.models.Sequential([\n",
    "    baseline_model,\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten()\n",
    "])\n",
    "\n",
    "print('****Model loaded successfully****\\n')\n",
    "\n",
    "models_history = train_model(model, 0.0001, 20, folds = 5, chkp_path = 'EfficientNetB0/chexpert/512px')    \n",
    "print('*'*20)\n",
    "print(f'Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EfficientNetB0 with resolution 512x512\n",
    "\n",
    "- Train_AUC: 0.7697\n",
    "- Valid_AUC: 0.7913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Functional)     (None, 10, 10, 1024)      7031232   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 102404    \n",
      "=================================================================\n",
      "Total params: 7,133,636\n",
      "Trainable params: 7,049,988\n",
      "Non-trainable params: 83,648\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 133s 381ms/step - loss: 1.2897 - auc: 0.6997 - val_loss: 1.1705 - val_auc: 0.7474\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 97s 324ms/step - loss: 1.2055 - auc: 0.7054 - val_loss: 1.2450 - val_auc: 0.7002\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 97s 323ms/step - loss: 1.1576 - auc: 0.7106 - val_loss: 1.2082 - val_auc: 0.6609\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 98s 325ms/step - loss: 1.1259 - auc: 0.7342 - val_loss: 1.1442 - val_auc: 0.7599\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 98s 326ms/step - loss: 1.0664 - auc: 0.7569 - val_loss: 1.0868 - val_auc: 0.7536\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 97s 324ms/step - loss: 1.1094 - auc: 0.7405 - val_loss: 1.3464 - val_auc: 0.7005\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 99s 330ms/step - loss: 1.0665 - auc: 0.7544 - val_loss: 1.1998 - val_auc: 0.7322\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 100s 334ms/step - loss: 1.0193 - auc: 0.7704 - val_loss: 1.2528 - val_auc: 0.7318\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 107s 326ms/step - loss: 1.2133 - auc_1: 0.7127 - val_loss: 1.1375 - val_auc_1: 0.7346\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 96s 321ms/step - loss: 1.1496 - auc_1: 0.7274 - val_loss: 1.1859 - val_auc_1: 0.7579\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 96s 320ms/step - loss: 1.1320 - auc_1: 0.7212 - val_loss: 1.6823 - val_auc_1: 0.6648\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 96s 320ms/step - loss: 1.1067 - auc_1: 0.7291 - val_loss: 1.1093 - val_auc_1: 0.7564\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 96s 320ms/step - loss: 1.0407 - auc_1: 0.7616 - val_loss: 1.0358 - val_auc_1: 0.7768\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 96s 320ms/step - loss: 1.0907 - auc_1: 0.7432 - val_loss: 1.1861 - val_auc_1: 0.6809\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 96s 319ms/step - loss: 1.0537 - auc_1: 0.7634 - val_loss: 1.0902 - val_auc_1: 0.7598\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 96s 320ms/step - loss: 0.9999 - auc_1: 0.7791 - val_loss: 1.1771 - val_auc_1: 0.7331\n",
      "\n",
      "********Training the model with validation fold 2********\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 110s 326ms/step - loss: 1.1625 - auc_2: 0.7433 - val_loss: 1.1586 - val_auc_2: 0.7614\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 96s 319ms/step - loss: 1.1012 - auc_2: 0.7410 - val_loss: 1.3244 - val_auc_2: 0.7073\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 96s 319ms/step - loss: 1.0596 - auc_2: 0.7594 - val_loss: 1.4126 - val_auc_2: 0.6997\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 96s 319ms/step - loss: 1.0258 - auc_2: 0.7707 - val_loss: 1.4196 - val_auc_2: 0.7088\n",
      "\n",
      "********Training the model with validation fold 3********\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 106s 325ms/step - loss: 1.1817 - auc_3: 0.7193 - val_loss: 1.0182 - val_auc_3: 0.8073\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 96s 320ms/step - loss: 1.0901 - auc_3: 0.7502 - val_loss: 1.0280 - val_auc_3: 0.7946\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 95s 318ms/step - loss: 1.0383 - auc_3: 0.7696 - val_loss: 1.1454 - val_auc_3: 0.7326\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 96s 319ms/step - loss: 1.0344 - auc_3: 0.7696 - val_loss: 1.0799 - val_auc_3: 0.7800\n",
      "\n",
      "********Training the model with validation fold 4********\n",
      "\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 106s 325ms/step - loss: 1.1298 - auc_4: 0.7489 - val_loss: 1.0706 - val_auc_4: 0.7699\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 96s 321ms/step - loss: 1.0524 - auc_4: 0.7667 - val_loss: 1.3699 - val_auc_4: 0.7118\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 96s 320ms/step - loss: 1.0124 - auc_4: 0.7801 - val_loss: 1.7046 - val_auc_4: 0.6451\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 96s 320ms/step - loss: 0.9771 - auc_4: 0.7909 - val_loss: 1.1974 - val_auc_4: 0.7431\n",
      "****Model trained successfully with mean AUC score of \n",
      " Train_AUC: 0.746\n",
      " Valid_AUC: 0.7738000000000002****\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    batchSize = 16\n",
    "    input_height, input_width = (320, 320)\n",
    "    pretrained_model = tf.keras.models.load_model('./saved_models/denseNet121/auc_0.7912/')\n",
    "    model = tf.keras.models.Sequential([\n",
    "        pretrained_model.layers[0],\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Flatten()\n",
    "    ])\n",
    "        \n",
    "    \n",
    "    print('****Model loaded successfully****\\n')\n",
    "\n",
    "    models_history = train_model(model, 0.0001, 20, folds = 5, chkp_path = 'DenseNet121/chexpert/320px')    \n",
    "    print(f'****Model trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]}****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_AUC: 0.746\n",
    "\n",
    "Valid_AUC: 0.7738000000000002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetB0 on 1080px images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:07.684631Z",
     "iopub.status.busy": "2021-07-09T16:38:07.684244Z",
     "iopub.status.idle": "2021-07-09T16:38:08.035364Z",
     "shell.execute_reply": "2021-07-09T16:38:08.034205Z",
     "shell.execute_reply.started": "2021-07-09T16:38:07.684599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6054"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing all the file names\n",
    "    \n",
    "batchSize = 2\n",
    "input_height, input_width = (1080,1080)\n",
    "train_dir = pathlib.Path(os.path.join(data_dir, f'{input_height}px','train','study'))\n",
    "train_image_paths = list(train_dir.glob('*.png'))\n",
    "\n",
    "len(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T16:38:08.037735Z",
     "iopub.status.busy": "2021-07-09T16:38:08.037281Z",
     "iopub.status.idle": "2021-07-09T16:38:12.622283Z",
     "shell.execute_reply": "2021-07-09T16:38:12.621133Z",
     "shell.execute_reply.started": "2021-07-09T16:38:08.03767Z"
    }
   },
   "outputs": [],
   "source": [
    "train_image_paths = [path for path in train_image_paths if str(path).split(os.sep)[-1].split('.')[0] in list(df.StudyInstanceUID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_153945128) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_153977703) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_153976047) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_153978212) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_153975494) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_153946520) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_153943937) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_153944112) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_153965765) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_153968643) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_153976644) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_153946353) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_153975355) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_153944668) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_153970752) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_153977659) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_153945322) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_153978073) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_153944954) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_153978256) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_153946311) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_153978809) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_153975538) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_153977150) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_153945280) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_153944147) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_153946179) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_153977520) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_153974849) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_153970402) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_153973420) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_153971764) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_153945357) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_153946102) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_153945893) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_153945935) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_153972867) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_153977106) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_153971625) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_153946597) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_153945776) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_153971258) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_153945741) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_153946388) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_153943513) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_153943708) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_153973237) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_153936663) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_153970796) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_153975032) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_153973743) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_153945163) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_153971119) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_153971302) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_153945698) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_153944264) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_153978626) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_153972823) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_153945566) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_153945489) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_153943743) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_153978765) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_153944710) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_153976600) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_153944306) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_153974479) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_153943471) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_153976967) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_153973926) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_153944341) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_153963989) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_153946562) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_153972317) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_153944919) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_153946144) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_153945531) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_153976461) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_153944069) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_153972684) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_153972178) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_153943902) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_153944473) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_153945970) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_153944745) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_153976091) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_153975908) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_153973882) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_153974988) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_153943665) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_153973376) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_153944516) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_153943860) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_153971808) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_153979132) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_153974435) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_153974296) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_153944877) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_153946714) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_153943548) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_153970613) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_153972361) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_153945086) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_153944551) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretrained_model = tf.keras.models.load_model('./saved_models/efficientNetB0/auc_0.7520/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layer input_1\n",
      "Loaded layer rescaling\n",
      "Loaded layer normalization\n",
      "Loaded layer stem_conv_pad\n",
      "Loaded layer stem_conv\n",
      "Loaded layer stem_bn\n",
      "Loaded layer stem_activation\n",
      "Loaded layer block1a_dwconv\n",
      "Loaded layer block1a_bn\n",
      "Loaded layer block1a_activation\n",
      "Loaded layer block1a_se_squeeze\n",
      "Loaded layer block1a_se_reshape\n",
      "Loaded layer block1a_se_reduce\n",
      "Loaded layer block1a_se_expand\n",
      "Loaded layer block1a_se_excite\n",
      "Loaded layer block1a_project_conv\n",
      "Loaded layer block1a_project_bn\n",
      "Loaded layer block2a_expand_conv\n",
      "Loaded layer block2a_expand_bn\n",
      "Loaded layer block2a_expand_activation\n",
      "Loaded layer block2a_dwconv_pad\n",
      "Loaded layer block2a_dwconv\n",
      "Loaded layer block2a_bn\n",
      "Loaded layer block2a_activation\n",
      "Loaded layer block2a_se_squeeze\n",
      "Loaded layer block2a_se_reshape\n",
      "Loaded layer block2a_se_reduce\n",
      "Loaded layer block2a_se_expand\n",
      "Loaded layer block2a_se_excite\n",
      "Loaded layer block2a_project_conv\n",
      "Loaded layer block2a_project_bn\n",
      "Loaded layer block2b_expand_conv\n",
      "Loaded layer block2b_expand_bn\n",
      "Loaded layer block2b_expand_activation\n",
      "Loaded layer block2b_dwconv\n",
      "Loaded layer block2b_bn\n",
      "Loaded layer block2b_activation\n",
      "Loaded layer block2b_se_squeeze\n",
      "Loaded layer block2b_se_reshape\n",
      "Loaded layer block2b_se_reduce\n",
      "Loaded layer block2b_se_expand\n",
      "Loaded layer block2b_se_excite\n",
      "Loaded layer block2b_project_conv\n",
      "Loaded layer block2b_project_bn\n",
      "Loaded layer block2b_drop\n",
      "Loaded layer block2b_add\n",
      "Loaded layer block3a_expand_conv\n",
      "Loaded layer block3a_expand_bn\n",
      "Loaded layer block3a_expand_activation\n",
      "Loaded layer block3a_dwconv_pad\n",
      "Loaded layer block3a_dwconv\n",
      "Loaded layer block3a_bn\n",
      "Loaded layer block3a_activation\n",
      "Loaded layer block3a_se_squeeze\n",
      "Loaded layer block3a_se_reshape\n",
      "Loaded layer block3a_se_reduce\n",
      "Loaded layer block3a_se_expand\n",
      "Loaded layer block3a_se_excite\n",
      "Loaded layer block3a_project_conv\n",
      "Loaded layer block3a_project_bn\n",
      "Loaded layer block3b_expand_conv\n",
      "Loaded layer block3b_expand_bn\n",
      "Loaded layer block3b_expand_activation\n",
      "Loaded layer block3b_dwconv\n",
      "Loaded layer block3b_bn\n",
      "Loaded layer block3b_activation\n",
      "Loaded layer block3b_se_squeeze\n",
      "Loaded layer block3b_se_reshape\n",
      "Loaded layer block3b_se_reduce\n",
      "Loaded layer block3b_se_expand\n",
      "Loaded layer block3b_se_excite\n",
      "Loaded layer block3b_project_conv\n",
      "Loaded layer block3b_project_bn\n",
      "Loaded layer block3b_drop\n",
      "Loaded layer block3b_add\n",
      "Loaded layer block4a_expand_conv\n",
      "Loaded layer block4a_expand_bn\n",
      "Loaded layer block4a_expand_activation\n",
      "Loaded layer block4a_dwconv_pad\n",
      "Loaded layer block4a_dwconv\n",
      "Loaded layer block4a_bn\n",
      "Loaded layer block4a_activation\n",
      "Loaded layer block4a_se_squeeze\n",
      "Loaded layer block4a_se_reshape\n",
      "Loaded layer block4a_se_reduce\n",
      "Loaded layer block4a_se_expand\n",
      "Loaded layer block4a_se_excite\n",
      "Loaded layer block4a_project_conv\n",
      "Loaded layer block4a_project_bn\n",
      "Loaded layer block4b_expand_conv\n",
      "Loaded layer block4b_expand_bn\n",
      "Loaded layer block4b_expand_activation\n",
      "Loaded layer block4b_dwconv\n",
      "Loaded layer block4b_bn\n",
      "Loaded layer block4b_activation\n",
      "Loaded layer block4b_se_squeeze\n",
      "Loaded layer block4b_se_reshape\n",
      "Loaded layer block4b_se_reduce\n",
      "Loaded layer block4b_se_expand\n",
      "Loaded layer block4b_se_excite\n",
      "Loaded layer block4b_project_conv\n",
      "Loaded layer block4b_project_bn\n",
      "Loaded layer block4b_drop\n",
      "Loaded layer block4b_add\n",
      "Loaded layer block4c_expand_conv\n",
      "Loaded layer block4c_expand_bn\n",
      "Loaded layer block4c_expand_activation\n",
      "Loaded layer block4c_dwconv\n",
      "Loaded layer block4c_bn\n",
      "Loaded layer block4c_activation\n",
      "Loaded layer block4c_se_squeeze\n",
      "Loaded layer block4c_se_reshape\n",
      "Loaded layer block4c_se_reduce\n",
      "Loaded layer block4c_se_expand\n",
      "Loaded layer block4c_se_excite\n",
      "Loaded layer block4c_project_conv\n",
      "Loaded layer block4c_project_bn\n",
      "Loaded layer block4c_drop\n",
      "Loaded layer block4c_add\n",
      "Loaded layer block5a_expand_conv\n",
      "Loaded layer block5a_expand_bn\n",
      "Loaded layer block5a_expand_activation\n",
      "Loaded layer block5a_dwconv\n",
      "Loaded layer block5a_bn\n",
      "Loaded layer block5a_activation\n",
      "Loaded layer block5a_se_squeeze\n",
      "Loaded layer block5a_se_reshape\n",
      "Loaded layer block5a_se_reduce\n",
      "Loaded layer block5a_se_expand\n",
      "Loaded layer block5a_se_excite\n",
      "Loaded layer block5a_project_conv\n",
      "Loaded layer block5a_project_bn\n",
      "Loaded layer block5b_expand_conv\n",
      "Loaded layer block5b_expand_bn\n",
      "Loaded layer block5b_expand_activation\n",
      "Loaded layer block5b_dwconv\n",
      "Loaded layer block5b_bn\n",
      "Loaded layer block5b_activation\n",
      "Loaded layer block5b_se_squeeze\n",
      "Loaded layer block5b_se_reshape\n",
      "Loaded layer block5b_se_reduce\n",
      "Loaded layer block5b_se_expand\n",
      "Loaded layer block5b_se_excite\n",
      "Loaded layer block5b_project_conv\n",
      "Loaded layer block5b_project_bn\n",
      "Loaded layer block5b_drop\n",
      "Loaded layer block5b_add\n",
      "Loaded layer block5c_expand_conv\n",
      "Loaded layer block5c_expand_bn\n",
      "Loaded layer block5c_expand_activation\n",
      "Loaded layer block5c_dwconv\n",
      "Loaded layer block5c_bn\n",
      "Loaded layer block5c_activation\n",
      "Loaded layer block5c_se_squeeze\n",
      "Loaded layer block5c_se_reshape\n",
      "Loaded layer block5c_se_reduce\n",
      "Loaded layer block5c_se_expand\n",
      "Loaded layer block5c_se_excite\n",
      "Loaded layer block5c_project_conv\n",
      "Loaded layer block5c_project_bn\n",
      "Loaded layer block5c_drop\n",
      "Loaded layer block5c_add\n",
      "Loaded layer block6a_expand_conv\n",
      "Loaded layer block6a_expand_bn\n",
      "Loaded layer block6a_expand_activation\n",
      "Loaded layer block6a_dwconv_pad\n",
      "Loaded layer block6a_dwconv\n",
      "Loaded layer block6a_bn\n",
      "Loaded layer block6a_activation\n",
      "Loaded layer block6a_se_squeeze\n",
      "Loaded layer block6a_se_reshape\n",
      "Loaded layer block6a_se_reduce\n",
      "Loaded layer block6a_se_expand\n",
      "Loaded layer block6a_se_excite\n",
      "Loaded layer block6a_project_conv\n",
      "Loaded layer block6a_project_bn\n",
      "Loaded layer block6b_expand_conv\n",
      "Loaded layer block6b_expand_bn\n",
      "Loaded layer block6b_expand_activation\n",
      "Loaded layer block6b_dwconv\n",
      "Loaded layer block6b_bn\n",
      "Loaded layer block6b_activation\n",
      "Loaded layer block6b_se_squeeze\n",
      "Loaded layer block6b_se_reshape\n",
      "Loaded layer block6b_se_reduce\n",
      "Loaded layer block6b_se_expand\n",
      "Loaded layer block6b_se_excite\n",
      "Loaded layer block6b_project_conv\n",
      "Loaded layer block6b_project_bn\n",
      "Loaded layer block6b_drop\n",
      "Loaded layer block6b_add\n",
      "Loaded layer block6c_expand_conv\n",
      "Loaded layer block6c_expand_bn\n",
      "Loaded layer block6c_expand_activation\n",
      "Loaded layer block6c_dwconv\n",
      "Loaded layer block6c_bn\n",
      "Loaded layer block6c_activation\n",
      "Loaded layer block6c_se_squeeze\n",
      "Loaded layer block6c_se_reshape\n",
      "Loaded layer block6c_se_reduce\n",
      "Loaded layer block6c_se_expand\n",
      "Loaded layer block6c_se_excite\n",
      "Loaded layer block6c_project_conv\n",
      "Loaded layer block6c_project_bn\n",
      "Loaded layer block6c_drop\n",
      "Loaded layer block6c_add\n",
      "Loaded layer block6d_expand_conv\n",
      "Loaded layer block6d_expand_bn\n",
      "Loaded layer block6d_expand_activation\n",
      "Loaded layer block6d_dwconv\n",
      "Loaded layer block6d_bn\n",
      "Loaded layer block6d_activation\n",
      "Loaded layer block6d_se_squeeze\n",
      "Loaded layer block6d_se_reshape\n",
      "Loaded layer block6d_se_reduce\n",
      "Loaded layer block6d_se_expand\n",
      "Loaded layer block6d_se_excite\n",
      "Loaded layer block6d_project_conv\n",
      "Loaded layer block6d_project_bn\n",
      "Loaded layer block6d_drop\n",
      "Loaded layer block6d_add\n",
      "Loaded layer block7a_expand_conv\n",
      "Loaded layer block7a_expand_bn\n",
      "Loaded layer block7a_expand_activation\n",
      "Loaded layer block7a_dwconv\n",
      "Loaded layer block7a_bn\n",
      "Loaded layer block7a_activation\n",
      "Loaded layer block7a_se_squeeze\n",
      "Loaded layer block7a_se_reshape\n",
      "Loaded layer block7a_se_reduce\n",
      "Loaded layer block7a_se_expand\n",
      "Loaded layer block7a_se_excite\n",
      "Loaded layer block7a_project_conv\n",
      "Loaded layer block7a_project_bn\n",
      "Loaded layer top_conv\n",
      "Loaded layer top_bn\n",
      "Loaded layer top_activation\n",
      "****Model loaded successfully****\n",
      "\n",
      "\n",
      "********Training the model with validation fold 0********\n",
      "\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 33, 33, 1280)      4048991   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 1280)      0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseC (None, 7, 7, 1280)        12800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 1280)        5120      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 3, 3, 1280)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "sigmoidLayer (Dense)         (None, 4)                 46084     \n",
      "=================================================================\n",
      "Total params: 4,112,995\n",
      "Trainable params: 4,068,416\n",
      "Non-trainable params: 44,579\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 962s 398ms/step - loss: 1.2670 - auc_5: 0.6921 - val_loss: 1.6838 - val_auc_5: 0.6662\n",
      "Epoch 2/20\n",
      "2400/2400 [==============================] - 956s 398ms/step - loss: 1.2025 - auc_5: 0.7213 - val_loss: 1.8341 - val_auc_5: 0.6892\n",
      "Epoch 3/20\n",
      "2400/2400 [==============================] - 956s 398ms/step - loss: 1.1633 - auc_5: 0.7414 - val_loss: 2.0846 - val_auc_5: 0.6629\n",
      "Epoch 4/20\n",
      "2400/2400 [==============================] - 940s 392ms/step - loss: 1.1428 - auc_5: 0.7535 - val_loss: 2.1245 - val_auc_5: 0.7045\n",
      "\n",
      "********Training the model with validation fold 1********\n",
      "\n",
      "Epoch 1/20\n",
      "2400/2400 [==============================] - 944s 389ms/step - loss: 1.2237 - auc_6: 0.7200 - val_loss: 1.9229 - val_auc_6: 0.6676\n",
      "Epoch 2/20\n",
      "2400/2400 [==============================] - 915s 381ms/step - loss: 1.1480 - auc_6: 0.7576 - val_loss: 1.9997 - val_auc_6: 0.6752\n",
      "Epoch 3/20\n",
      " 129/2400 [>.............................] - ETA: 14:37 - loss: 1.2111 - auc_6: 0.7327"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-afeb233cb0d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'****Model loaded successfully****\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodels_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchkp_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'EfficientNetB0/chexpert/{input_height}px'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\nModel trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-bdf96d74f041>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(baseline_model, max_lr, epochs, folds, chkp_path, dropout_rate, label_smoothing)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Fitting the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# Getting the best models results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1186\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \"\"\"\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    335\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m       \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1101\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    513\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \"\"\"\n\u001b[0;32m   1093\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1095\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "baseline_model = change_model_input_size(pretrained_model, input_height, input_width) \n",
    "model = tf.keras.models.Sequential([\n",
    "    baseline_model,\n",
    "    tf.keras.layers.MaxPool2D(strides = (2,2)),\n",
    "    tf.keras.layers.DepthwiseConv2D(3, 2, activation = None),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(strides = (2,2)),\n",
    "    tf.keras.layers.Flatten()\n",
    "])\n",
    "\n",
    "print('****Model loaded successfully****\\n')\n",
    "\n",
    "models_history = train_model(model, 0.0001, 20, folds = 5, chkp_path = f'EfficientNetB0/chexpert/{input_height}px')    \n",
    "print('*'*20)\n",
    "print(f'\\nModel trained successfully with mean AUC score of \\n Train_AUC: {models_history[\"auc\"]}\\n Valid_AUC: {models_history[\"val_auc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model accuracies did not improve for high resolution images. Tests will be conducted one more time for model pretrained on chexpert dataset using  high resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_19",
   "language": "python",
   "name": "covid_19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
